---
title: Quantile Autoregressive Distributed Lag model Global selection
author:
  - name: Taís 
    surname: Bellini
    email: tais38@gmail.com
    label: e1
    addressLabel: A
affiliation:
  - label: A
    name: Instituto de Matemática e Estatística, Universidade Federal do Rio Grande do Sul
    authorsLabels: e1
abstract: |
  The abstract should summarize the contents of the paper.
  It should be clear, descriptive, self-explanatory and not longer
  than 200 words. It should also be suitable for publication in
  abstracting services. Formulas should be used as sparingly as
  possible within the abstract. The abstract should not make
  reference to results, bibliography or formulas in the body
  of the paper---it should be self-contained.

  This is a sample input file.  Comparing it with the output it
  generates can show you how to produce a simple document of
  your own.
keyword-subclass: | 
 \begin{keyword}[class=MSC2020] % It must be define for aap, aop, aos journals. For aoas, sts is not used
 \kwd[Primary ]{00X00}
 \kwd{00X00}
 \kwd[; secondary ]{00X00}
 \end{keyword}
keywords: 
  - quantile regression
  - time series
  - Estimation
  - Regularization
  - adaLasso

predefined-theoremstyle: true # use in section Environments for Axiom, Theorem, etc
bibliography: ims.bib
biblio-style: imsart-nameyear # alternative: imsart-number
output:
  rticles::ims_article:
    journal: aoas # aap, aoas, aop, aos, sts. See documentation
    toc: false # Please use for articles with 50 pages and more
    fig_caption: yes
header-includes:
  - \DeclareMathOperator{\EX}{\mathbb{E}}
  - \DeclareMathOperator{\Z}{\mathbb{Z}}
  - \DeclareMathOperator{\R}{\mathbb{R}}
  - \DeclareMathOperator*{\argmax}{arg\,max}
---

# Introduction

<!---
Todos os comentários estão marcados com `??'
?? Acredito que podes começar com uma visão puramente ``populacional'' de regressão quantílica, deixando a questão sobre estimação para um segundo momento... é uma questão de gosto: eu, em geral, prefiro ``desemaranhar'' esses dois aspectos metodológicos. Então eu começaria apresentando o modelo (possivelmente pela equação 1.3) citando Galvão et al. primeiro, que é uma generalização de Koenker and Xiao, que por sua vez são uma generalização para um cenário time series da regressão quantílica para dados ``cross section'' do Koenker and Bassett. É importante aqui:
1. ressaltar a flexibilidade/parcimônia de um modelo do tipo 
Q_{Y_t}(\tau | passado) = Z_{t-1}'\beta(\tau)
(onde Z_{t-1} é um vetor cujas componentes são defasagens de Y e X) pois te permite capturar nuances na estrutura de dependência entre Z e Y
2. chamar a atenção para o problema de ``quantile crossing'' ou, mais geralmente, para o requerimento implícito de que a equação acima seja uma função não-decrescente de \tau
3. chamar a atenção para o fato de que a equação (1.3) especifica a dinâmica multivariada apenas parcialmente: por exemplo, se (X_t) é univariado, então (tomando p=q=1) tu obtens uma dinâmica bivariada $(Y_t,X_t)_{t\in\mathbb{Z}}$ em que a primeira componente ``se atualiza'' de acordo com a distribuição de probabilidades (condicional) correspondente à função quantílica
Q_{Y_{t}}(\tau | passado) = c(\tau) + \alpha_1(\tau)Y_{t-1} + \beta_1(\tau)X_{t-1}

Enfim, o modelo é flexível e parcimonioso, mas requer que certas restrições sejam impostas.

--->

Quantile regression, introduced in @Koenker1978, provides an estimation for the conditional distribution of the response variable $Y$ on the vector of covariates $\mathbf{X}$ at different quantile levels, denoted here as $\tau\in(0,1)$, thus, offering an approximation for the whole conditional distribution. It offers a more robust estimation for outliers, as opposed to classical linear regression that only evaluates the conditional mean at a specific location. [@Davino2014] In @Koenker2005, the Quantile Autoregressive (QAR) model is introduced, where the $\tau$-th conditional quantile function of the response variable $Y_t$ is expressed as a linear function of the lagged values of $Y$ and all of the autoregressive coefficients can depend on $\tau$. Quantile regression estimation for other traditional time series models are described in @Koenker2018, such as ARMA and ADL models. @Galvao2013 generalize the QAR model to a framework that introduces exogenous stationary covariates, the Quantile Autoregressive Distributed Lag (QADL) model, demonstrating it can deliver good insights on asymmetric dynamics.

In quantile regression, we get the conditional quantile function for a determined $\tau$. Therefore, for each desired quantile, there will be a regression model and estimation. This brings complexity to certain operations, since we will have $M$ different estimations, where $M$ is the number of quantiles we wish to evaluate. For example, if we want to perform variable selection using regularization techniques, such as LASSO or AdaLASSO, we might have different variables selected for each $\tau$. @Sottile2020 proposes an approach for quantile regression that estimates the coefficients for a given grid of $\tau$'s in one single minimization problem. With this approach, it is possible to perform a global selection of variables for a grid of quantiles. @Sottile2020 studies global coefficient estimation and variable selection in cross-sectional data using LASSO, demonstrating being able to efficiently approximate the true model with high probability. 

In this work, we propose a global coefficient estimation and variable selection method based on the estimator presented in @Sottile2020. The proposal introduces the group lasso penalty, suggested in @Yuan2006, and is applied in a Quantile Autoregressive Distributed Lag (QADL) model. Furthermore, since we are in a time series context, the variable selection penalization applies higher penalties to higher lags, as proposed in @Konzen2016. The paper is organizes as follows: Chapter \ref{fundamentals} describes the main concepts used in this work and the proposed estimator, Chapter \ref{methodology} explains how the QADL process was generated and how the simulation process was setup, Chapter \ref{results} analyses the results of the simulation and comparison with other methods, and Chapter \ref{conclusion} provides a final discussion enlightening future work.

# Fundamentals \label{fundamentals}

In this section, the fundamental concepts used in this work are presented. We start defining the Quantile Autoregressive Distributed Lag Model, describe the proposed estimator and express the variable selection method.

## Quantile Autoregressive Distributed Lag Model (QADL)

Consider an autoregressive-distributed lag model described by the equation:
\begin{equation}
Y_t = c + \alpha_1Y_{t-1} +  \cdots + \alpha_pY_{t-p} + \theta_1\mathbf{X}_{t-1}^\intercal + \cdots + \theta_q\mathbf{X}_{t-q}^\intercal + \varepsilon_t 
\label{eq:adl}
\end{equation}
where $t = 1, ..., T$, $Y_t$ is the response variable, $Y_{t-j}$ the lag of the response variable and $\mathbf{X}_{t-q}$ is the lagged covariates vector with dimension $D$. $\varepsilon_t$ is a white noise.

As in @Koenker2006, we are interested in studying a class of quantile autoregressive models which coefficients can be dependent of $\tau$. Therefore, lets consider the following process, with ${U_t}$ as a sequence of i.i.d standard uniform random variables, $c$, $\alpha$ and $\theta$ as unknown functions $[0,1] \rightarrow \mathbb{R}$ to be estimated:
\begin{equation}
Y_t = c(U_t) + \alpha_1(U_t)Y_{t-1} +  \cdots + \alpha_p(U_t)Y_{t-p} + \theta_1(U_t)\mathbf{X}_{t-1}^\intercal + \cdots + \theta_q(U_t)\mathbf{X}_{t-q}^\intercal
\label{eq:adltau}
\end{equation}

Then, given that for any monotone increasing function $g$ and a standard uniform random variable $U$ the following is true:

$$
Q_{g(U)}(\tau) = g(Q_U(\tau)) = g(\tau)
$$

Assuming the right side of the equation \ref{eq:adltau} is monotone and increasing on $U_t$, we can say that the $\tau_{th}$ conditional quantile function of $Y_t$ is:

\begin{equation}
Q_{Y_t}(\tau|\Im_t) =  c(\tau) + \alpha_1(\tau)Y_{t-1} + \cdots + \alpha_p(\tau)Y_{t-p} + \theta_1(\tau)\mathbf{X}_{t-1}^\intercal + \cdots + \theta_q(\tau)\mathbf{X}_{t-q}^\intercal
\label{eq:qadl}
\end{equation}
where $\Im_t$ is the $\sigma$-field generated by ${Y_s, s\leq t}$. 

(Quantile monotonicity @Koenker2006)

Now let $\mathbf{Z_t}^\intercal$ be the covariates matrix with dimension $D$ and $\beta(\tau)$ a vector of the coefficients that describe the relationship between the covariates $\mathbf{Z}$ and the $\tau$-th quantile of the response variable, $\tau \in (0,1)$.

Then, equation \ref{eq:qadl} can be expressed in the form of 
\begin{equation}
Q_{Y_t}(\tau|\Im_t) = \mathbf{Z_t}^\intercal\beta(\tau)
\label{eq:qadlred}
\end{equation}

Given the QADL model described above, $\mathbf{Z_t}^\intercal$ and $\beta(\tau)$ are, respectively, 
$$
\mathbf{Z_t}^\intercal = (1, Y_{t-1}, \dots, Y_{t-p}, \mathbf{X}_{t-1}, \dots, \mathbf{X}_{t-p})
$$ 
and
$$
\beta(\tau)^\intercal = (c(\tau), \alpha_1(\tau), \dots, \alpha_p(\tau), \theta_1(\tau), \cdots , \theta_q(\tau))
$$
This representation, besides parsimonious, allows us to capture the nuances in the dependency structure of $\mathbf{Z}$ and $Y$.

## Global coefficient estimation and variable selection

In a standard quantile regression (QR) method, we would estimate the $D$-dimensional vector $\beta$ for each $\tau$ one at a time minimizing the expected value of the check function $\rho_\tau$:
$$
\hat{\beta}(\tau) = \arg\min_b \sum_{i=1}^N{\rho_\tau(Y_i - \mathbf{Z_i}^\intercal b)}
$$
where $\rho_\tau(v) = v(\tau - \mathbb{I}_{[v<0]})$. Thus, to find the $\beta(\tau)$ function that minimizes the loss function for all $\tau$, we have the following minimization problem:

\begin{equation}
\hat{\beta}(\tau) = \arg\min_b \sum_{i=1}^N{\int_{0}^{1}\rho_\tau(Y_i - \mathbf{Z_i}^\intercal b) d\tau}
\label{eq:betafunc}
\end{equation}

@Frumento2016 suggested a different approach: modeling the coefficient functions $\beta(\tau)$ as parametric functions of the order of the quantile. Consider $\varphi$ a vector of model parameters, then we can describe the quantile function as: 

\begin{equation}
Q(\tau|\mathbf{Z},\varphi) = \mathbf{Z}^\intercal\beta(\tau|\varphi)
\label{eq:qrcm}
\end{equation}

As stated in @Sottile2020, this method improves the efficiency and interpretation of the results, allowing us to maintain the quantile regression structure in \ref{eq:qadlred} but modeling it parametrically. 

To model $\beta(\tau|\varphi)$, a good practice is to use a flexible model, such as a $k$-th degree polynomial function:

$$
\beta_j(\tau|\varphi) = \varphi_{j0} + \varphi_{j1}\tau + \cdots + \varphi_{jk}\tau^k
$$
where $j = 1, \dots,D$ and $D$ is the number of covariates of the model. $\varphi$ will have dimensions $Q\times(k+1)$, so each covariate has $k+1$ associated parameters. 

### Estimation

Define $\varphi(\tau)$ a set of $L$ known functions of $\tau$, with $L = 1,2,\dots$, and $\delta$ a matrix with dimensions $D \times L$, where $D$ is the number of covariates of the model. Now, consider the following linear parametrization: 

\begin{equation}
\beta(\tau|\varphi) = \delta\varphi(\tau)
\label{eq:betac}
\end{equation}
where $\varphi(\tau)$ is a $L \times M$ matrix, the resulting dimensions of $\beta(\tau|\varphi)$ is $D \times M$, being $M$ the size of the quantile grid we are estimating. Thus, we have that:

\begin{equation}
\beta_j(\tau|\varphi) = \delta_{1j}\phi_1(\tau) + \dots + \delta_{Mj}\phi_M(\tau)
\label{eq:betaj}
\end{equation}

Hence, we can rewrite equation \ref{eq:qrcm} as:

\begin{equation}
Q(\tau|\mathbf{Z},\varphi) = \mathbf{Z}^\intercal\delta\varphi(\tau)
\label{eq:qrcme}
\end{equation}

Since $\varphi(\tau)$ is known, our goal is to find $\delta$ that minimizes the loss function of the standard QR, so \ref{eq:betafunc} can be rewritten as:

\begin{equation}
\hat{\beta}(\tau|\varphi) = \arg\min_{\delta} \sum_{i=1}^n{\int_{0}^{1}\rho_\tau(Y_i - \mathbf{Z_i}^\intercal \delta\varphi(\tau))d\tau}
\label{eq:betafunc2}
\end{equation}


### Variable selection

To perform a LASSO variable selection, equation \ref{eq:betafunc2} is modified to introduce a $l^1$-norm penalizing factor (@Sottile2020):

\begin{equation}
\hat{\beta}(\tau|\varphi) = \arg\min_{\delta} \sum_{i=1}^n{\int_{0}^{1}\rho_\tau(Y_i - \mathbf{Z_i}^\intercal \delta\varphi(\tau))d\tau} + \lambda \sum_{j=1}^{D}\sum_{h=1}^{L}|\delta_{jh}|
\label{eq:betafuncpen}
\end{equation}
with $\lambda > 0$ as a tuning parameter. Thus, minimization is done subject to $\sum_{j=1}^{D}\sum_{h=1}^{L}|\delta_{jh}| \leq s$, where $s$ is a defined bound to the model parameters. 

An alternative to overcome the inconsistencies that LASSO selection can present is the adaLASSO method proposed by @Zou2006, where different weights $\omega_j$ are applied to different coefficients. In this approach, variables with lower estimated coefficient, which indicate they are less relevant, will have a great weight $\omega_j$ resulting in a higher penalty. In this scenario, \ref{eq:betafuncpen} would have an extra parameter to represent the weight $\omega_j$:

\begin{equation}
\hat{\beta}(\tau|\varphi) = \arg\min_{\delta} \sum_{i=1}^n{\int_{0}^{1}\rho_\tau(Y_i - \mathbf{Z_i}^\intercal \delta\varphi(\tau))d\tau} + \lambda \sum_{j=1}^{D}\sum_{h=1}^{L}\omega_j|\delta_{jh}|
\label{eq:betafuncpenAda}
\end{equation}

The $\delta$ parameter being estimated is a $D \times L$ matrix, where D is the number of covariates in the model, so a covariate is represented in a row. In this scenario, it is relevant to use the idea from @Yuan2006 to penalize the variable in the $\delta$ matrix in a grouped manner. The adaptive group lasso estimator applies a $l^2$-norm penalization:

\begin{equation}
\hat{\beta}(\tau|\varphi) = \arg\min_{\delta} \sum_{i=1}^n{\int_{0}^{1}\rho_\tau(Y_i - \mathbf{Z_i}^\intercal \delta\varphi(\tau))d\tau} + \lambda \sum_{j=1}^{D}\omega_j\Vert\delta_{j}\Vert
\label{eq:betafuncpenAdaGroup}
\end{equation}
where 
$$
\Vert\delta_{j}\Vert = \sqrt{\sum_{h=1}^L\delta_{j,h}^2}
$$

In a time series context, we are estimating the coefficients for each lagged variable, thus, (Park and Sakaori 2013) consider applying higher weights for higher lags, as it is usual that more recent variables are more relevant. @Konzen2016 apply this idea to propose the WLadaLASSO, that has its original form described as $\omega_j = \left(|\hat{\beta_j}|e^{-\alpha t}\right)^{-p}$, with $p>0$, $\alpha \geq 0$ and $t$ representing the lag order. For \ref{eq:betafuncpenAdaGroup}, we will have that $\omega_j = \left(\Vert\hat{\delta}_{j}\Vert e^{-\alpha t}\right)^{-p}$.


# Methodology \label{methodology}

## Simulation

As stated in Section 2, when studying quantile regression, we need to assume certain restrictions to avoid situations like quantile-crossing. To allow  more flexibility on the $\beta(U_t)$ parameters, we usually restrict the support of the covariates to be bounded in $[0, +\infty]$. In a time series context, where we are modeling a process like \ref{eq:adltau}, we need to impose these restriction not only to the right side of the equation, as usually enough in linear quantile regression, but also in the response variable, as $Y_t$ and $Y_{t-1}$ are equal in distribution. To simulate \ref{eq:adltau} ensuring that the right-hand side of the equation is monotone and increasing in $U_t$, we will use a family of hyperplanes, as discussed in @Horta2021. 

Consider $v_d:(0,1)\rightarrow \R$ where $d\in \{0,1\}^2$ as non-decreasing and left-continuous functions that satisfy: 

\begin{longlist}
\item[1.]
$v_{11} = v_{10} +v_{01} - v_{00}$
\item[2.]
$\tau\rightarrow v_{11}(\tau)$ is non-decreasing.
\end{longlist}

Now, if $\mathbf{X}$ is a random vector in ${\R}^3$ and $U \sim U(0,1)$ is a scalar random variable independent of $\mathbf{X}$, then we can define the random variable $Y$ as
$$
Y = v_{00}(U) + (v_{10}(U)-v_{00}(U))X_2 + (v_{01}(U) - v_{00}(U))X_3 
$$
and it will satisfy, for any $\tau \in (0,1)$ and all $x$,  
$$
Q_{Y|X}(\tau|x) = \beta_0(\tau) +  \beta_1(\tau)x_2 + \beta_2(\tau)x_3
$$
where $\beta_0(\tau) = v_{00}$,  $\beta_1(\tau) = v_{10}-v_{00}$ and $\beta_2(\tau) = v_{01} - v_{00}$.

### The generated QADL models

To study the proposed estimator, we simulate the Y variable with the following model:
$$
Q_{Y_t}(U_t|\Im_{t-1}) = c(U_t) + \alpha_1(U_t)Y_{t-1} + \theta_1(U_t)X_{t-1}
$$
and 
$$
Q_{X_t}(U_t|\Im_{t-1}) = c(U_t) + \alpha_1(U_t)X_{t-1} + \theta_1(U_t)Y_{t-1}
$$
where $v_{00}(\tau) = QBeta(\tau,3,1)$, $v_{10}(\tau) = QBeta(\tau,1,3)$, $v_{01}(\tau) = QBeta(\tau,5,1)$, $c(U_t) = v_{00}(U_t)$, $\alpha_1(U_t) = v_{10}(U_t) - v_{00}(U_t)$ and $\theta_1(U_t) = v_{01}(U_t) - v_{00}(U_t)$.

```{r, echo=FALSE, message=F, warning=F}
set.seed(205650)
source('qardl-gen.r')
qardl = simulate_qardl(N = 10001)
Y = qardl$Y
Z = qardl$Z
```

Figure \ref{fig:vd} illustrate how the chosen $v_d$ function behave on the grid of $\tau$'s. $v_{00}$, $v_{01}$ and $v_{10}$ are the quantile functions of the Beta distribution given the parameters described above. Then, $v_{11}$ is the resulting function of the operation $v_{10} +v_{01} - v_{00}$.

A simulation of a time series of 10000 observations using the method described above is illustrated in Figure \ref{fig:Y}.

Figure \ref{fig:Ycorr} present the autocorrelation function and the partial autocorrelation function of the process generated by the method described, the correlation between $Yt$ and $Y_{t-1}$ as well as the correlation between $Yt$ and $X_{t-1}$. 

```{r figs, echo=FALSE, warning=F,fig.cap="\\label{fig:vd}Chosen $v_d$ Functions Behavior"}

par(mfrow=c(2,2))
# Conditional density of Y[t] given Y[t-1] = 1 and X[t-1]=0
dbplot(a10,b10, title="Yt given Yt-1=1 and Xt-1=0", y_lab="v10 function")
# Conditional density of Y[t] given Y[t-1] = 0 and X[t-1]=1
dbplot(a01,b01, title="Yt given Yt-1=0 and Xt-1=1", y_lab="v01 function")
# Conditional density of Y[t] given Y[t-1]=0 and X[t-1]=0
dbplot(a00,b00, title="Yt given Yt-1=0 and Xt-1=0", y_lab="v00 function")
# Conditional density of Y[t] given Y[t-1]=1 and X[t-1]=1
tau.grid = seq(from=.01,to=.99, by=.02)
plot(v11(tau.grid), 1/q11(tau.grid), xlim=c(0,1), main = "Yt given Yt-1=0 and Xt-1=0", type="l", ylab="v11", xlab="tau.grid")
```


```{r Yfigs, echo=FALSE, warning=F, fig.height=3,fig.cap="\\label{fig:Y}Y time series"}
par(mfrow=c(1,2))
hist(Y, border=NA, breaks="FD")
ts.plot(Y)
```


```{r Ycorr, echo=FALSE, warning=F, fig.width=7,fig.height=6, fig.cap="\\label{fig:Ycorr}ACF,PACF and correlations of Yt and Xt"}

T = length(Y)
par(mfrow=c(2,2))
acf(Y, lwd=16, lend=3, col='gray')
pacf(Y, lwd=16, lend=3, col='gray')
plot(Y[2:T]~Y[1:(T-1)], pch=16, col=rgb(0,0,0,.4), ylab="Yt", xlab="Yt-1")
plot(Y[2:T]~Z[1:(T-1)], pch=16, col=rgb(0,0,0,.4), ylab="Yt", xlab="Xt-1")
```

## Estimation and evaluation

To globally estimate the functions $c(\tau)$, $\alpha_1(\tau)$ and $\theta_1(\tau)$ for a given grid of $\tau$s, the CVXR R-package (@cvxr2020) for convex optimization was used to minimize the objective function described in \ref{eq:betafuncpenAda}. 

To test the proposed approach, we used 3 Monte Carlo simulations with 200 replications:

\begin{longlist}
\item[11]
A simulated dataset with only $Y_{t-1}$ and $X_{t-1}$ as covariates;
\item[2.]
A dataset with two lags of each covariate Y and X;
\item[3.]
A dataset with 10 lags of each covariate Y and X;
\end{longlist}

We analyzed two aspects: the coefficients estimation and the chosen variables by the variable selection. We compared the results with a regular quantile regression approach, the _piqr_ function without weights implemented in @Sottile2020.
The metrics used were SSR on two dimensions: by variable and quantile.


- Monte Carlo
- global estimation using Sottile with WLadaLASSO penalization
- Evaluation

# Results \label{results}

## Database with one lag

## Database with two lags

## Database with ten lags

## Lists

The following is an example of an *itemized* list, two levels deep.
 
* This is the first item of an itemized list.  Each item
  in the list is marked with a "tick." The document
  style determines what kind of tick mark is used.
* This is the second item of the list.  It contains another
  list nested inside it.
  - This is the first item of an itemized list that
    is nested within the itemized list.
  - This is the second item of the inner list.  \LaTeX\
    allows you to nest lists deeper than you really should.
- This is the third item of the list.

The following is an example of an *enumerated* list of one level.

\begin{longlist}
\item This is the first item of an enumerated list.
\item This is the second item of an enumerated list.
\end{longlist}

The following is an example of an *enumerated* list, two levels deep.
\begin{longlist}
\item[1.]
This is the first item of an enumerated list.  Each item
in the list is marked with a ``tick.''.  The document
style determines what kind of tick mark is used.
\item[2.]
This is the second item of the list.  It contains another
list nested inside of it.
\begin{longlist}
\item
This is the first item of an enumerated list that
is nested within.  
\item
This is the second item of the inner list.  \LaTeX\
allows you to nest lists deeper than you really should.
\end{longlist}
This is the rest of the second item of the outer list.
\item[3.]
This is the third item of the list.
\end{longlist}

## Punctuation

Dashes come in three sizes: a hyphen, an intra-word dash like "$U$-statistics" or "the time-homogeneous model";
a medium dash (also called an "en-dash") for number ranges or between two equal entities like "1--2" or "Cauchy--Schwarz inequality";
and a punctuation dash (also called an "em-dash") in place of a comma, semicolon,
colon or parentheses---like this.

Generating an ellipsis \ldots\ with the right spacing
around the periods requires a special command.

# Fonts

Please use text fonts in text mode, e.g.:
\begin{itemize}
\item[]\textrm{Roman}
\item[]\textit{Italic}
\item[]\textbf{Bold}
\item[]\textsc{Small Caps}
\item[]\textsf{Sans serif}
\item[]\texttt{Typewriter}
\end{itemize}
Please use mathematical fonts in mathematical mode, e.g.:
\begin{itemize}
\item[] $\mathrm{ABCabc123}$
\item[] $\mathit{ABCabc123}$
\item[] $\mathbf{ABCabc123}$
\item[] $\boldsymbol{ABCabc123\alpha\beta\gamma}$
\item[] $\mathcal{ABC}$
\item[] $\mathbb{ABC}$
\item[] $\mathsf{ABCabc123}$
\item[] $\mathtt{ABCabc123}$
\item[] $\mathfrak{ABCabc123}$
\end{itemize}
Note that \verb|\mathcal, \mathbb| belongs to capital letters-only font typefaces.

# Notes

Footnotes[^1]
pose no problem.[^2]

[^1]: This is an example of a footnote.
[^2]: Note that footnote number is after punctuation.

# Quotations

Text is displayed by indenting it from the left margin. There are short quotations

> This is a short quotation.  It consists of a
> single paragraph of text.  There is no paragraph
> indentation.

and longer ones.

<!-- custom blocks syntax https://bookdown.org/yihui/rmarkdown-cookbook/custom-blocks.html -->

::: {.quotation data-latex=""}
This is a longer quotation.  It consists of two paragraphs
of text.  The beginning of each paragraph is indicated
by an extra indentation.

This is the second paragraph of the quotation.  It is just
as dull as the first paragraph.
:::

# Environments

## Examples for *`plain`-style environments*

::: {.axiom data-latex=""}
\label{ax1}
This is the body of Axiom \ref{ax1}.
:::

::: {.proof data-latex=""}
This is the body of the proof of the axiom above.
:::

::: {.claim data-latex=""}
\label{cl1}
This is the body of Claim \ref{cl1}. Claim \ref{cl1} is numbered after
Axiom \ref{ax1} because we used \verb|[axiom]| in \verb|\newtheorem|.
:::

::: {.theorem data-latex=""}
\label{th1}
This is the body of Theorem \ref{th1}. Theorem \ref{th1} numbering is
dependent on section because we used \verb|[section]| after \verb|\newtheorem|.
:::

::: {.theorem data-latex="[Title of the theorem]"}
\label{th2}
This is the body of Theorem \ref{th2}. Theorem \ref{th2} has additional title.
:::

::: {.lemma data-latex=""}
\label{le1}
This is the body of Lemma \ref{le1}. Lemma \ref{le1} is numbered after
Theorem \ref{th2} because we used \verb|[theorem]| in \verb|\newtheorem|.
:::

::: {.proof data-latex="[Proof of Lemma \ref{le1}]"}
This is the body of the proof of Lemma \ref{le1}.
:::

## Examples for *`remark`*-style environments
::: {.definition data-latex=""}
\label{de1}
This is the body of Definition \ref{de1}. Definition \ref{de1} is numbered after
Lemma \ref{le1} because we used \verb|[theorem]| in \verb|\newtheorem|.
:::

::: {.example data-latex=""}
This is the body of the example. Example is unnumbered because we used \verb|\newtheorem*|
instead of \verb|\newtheorem|.
:::

::: {.fact data-latex=""}
This is the body of the fact. Fact is unnumbered because we used \verb|\newtheorem*|
instead of \verb|\newtheorem|.
:::

# Tables and figures
Cross-references to labeled tables: As you can see in Table\ref{tab:mtc}
and also in Table\ref{parset}.

```{r mtc, echo = FALSE}
knitr::kable(mtcars, caption = "Table caption", format = "latex", vline = "", linesep = "")
```

\begin{table}
\caption{Sample posterior estimates for each model}
\label{parset}
%
\begin{tabular}{@{}lcrcrrr@{}}
\hline
&& & &\multicolumn{3}{c}{Quantile} \\
\cline{5-7}
Model &Parameter &
\multicolumn{1}{c}{Mean} &
Std. dev.&
\multicolumn{1}{c}{2.5\%} &
\multicolumn{1}{c}{50\%}&
\multicolumn{1}{c@{}}{97.5\%} \\
\hline
{Model 0} & $\beta_0$ & $-$12.29 & 2.29 & $-$18.04 & $-$11.99 & $-$8.56 \\
          & $\beta_1$  & 0.10   & 0.07 & $-$0.05  & 0.10   & 0.26  \\
          & $\beta_2$   & 0.01   & 0.09 & $-$0.22  & 0.02   & 0.16  \\[6pt]
{Model 1} & $\beta_0$   & $-$4.58  & 3.04 & $-$11.00 & $-$4.44  & 1.06  \\
          & $\beta_1$   & 0.79   & 0.21 & 0.38   & 0.78   & 1.20  \\
          & $\beta_2$   & $-$0.28  & 0.10 & $-$0.48  & $-$0.28  & $-$0.07 \\[6pt]
{Model 2} & $\beta_0$   & $-$11.85 & 2.24 & $-$17.34 & $-$11.60 & $-$7.85 \\
          & $\beta_1$   & 0.73   & 0.21 & 0.32   & 0.73   & 1.16  \\
          & $\beta_2$   & $-$0.60  & 0.14 & $-$0.88  & $-$0.60  & $-$0.34 \\
          & $\beta_3$   & 0.22   & 0.17 & $-$0.10  & 0.22   & 0.55  \\
\hline
\end{tabular}
%
\end{table}


```{r,echo=FALSE,fig.cap="Figure caption\\label{penG}"}
plot(1:10)
```

Sample of cross-reference to figure.
Figure\ref{penG} shows that it is not easy to get something on paper.

# Equations and the like

Two equations:
\begin{equation}
    C_{s}  =  K_{m} \frac{\mu/\mu_{x}}{1-\mu/\mu_{x}} \label{ccs}
\end{equation}
and
\begin{equation}
    G = \frac{P_{\mathrm{opt}} - P_{\mathrm{ref}}}{P_{\mathrm{ref}}}  100(\%).
\end{equation}

Equation arrays:
\begin{eqnarray}
  \frac{dS}{dt} & = & - \sigma X + s_{F} F,\\
  \frac{dX}{dt} & = &   \mu    X,\\
  \frac{dP}{dt} & = &   \pi    X - k_{h} P,\\
  \frac{dV}{dt} & = &   F.
\end{eqnarray}
One long equation:
\begin{eqnarray}
 \mu_{\text{normal}} & = & \mu_{x} \frac{C_{s}}{K_{x}C_{x}+C_{s}}  \nonumber\\
                     & = & \mu_{\text{normal}} - Y_{x/s}\bigl(1-H(C_{s})\bigr)(m_{s}+\pi /Y_{p/s})\\
                     & = & \mu_{\text{normal}}/Y_{x/s}+ H(C_{s}) (m_{s}+ \pi /Y_{p/s}).\nonumber
\end{eqnarray}

<!-- %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -->
<!-- %% Example with single Appendix:            %% -->
<!-- %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -->

::: {.appendix data-latex=""}
# Title {.unnumbered #appn}

Appendices should be provided in \verb|{appendix}| environment,
before Acknowledgements.

If there is only one appendix,
then please refer to it in text as \ldots\ in the \hyperref[appn]{Appendix}.
:::

<!-- %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -->
<!-- %% Example with multiple Appendixes:        %% -->
<!-- %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -->

::: {.appendix data-latex=""}
# Title of the first appendix {#appA}
If there are more than one appendix, then please refer to it
as \ldots\ in Appendix \ref{appA}, Appendix \ref{appB}, etc.

# Title of the second appendix {#appB}

## First subsection of Appendix \protect\ref{appB}
Use the standard \LaTeX\ commands for headings in \verb|{appendix}|.
Headings and other objects will be numbered automatically.

\begin{equation}
\mathcal{P}=(j_{k,1},j_{k,2},\dots,j_{k,m(k)}). \label{path}
\end{equation}

Sample of cross-reference to the formula (\ref{path}) in Appendix \ref{appB}.
:::

<!-- %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -->
<!-- %% Support information (funding), if any,   %% -->
<!-- %% should be provided in the                %% -->
<!-- %% Acknowledgements section.                %% -->
<!-- %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -->

## Acknowledgements {.unnumbered}

The authors would like to thank the anonymous referees, an Associate
Editor and the Editor for their constructive comments that improved the
quality of this paper.

The first author was supported by NSF Grant DMS-??-??????.

The second author was supported in part by NIH Grant ???????????.


<!-- %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -->
<!-- %% Supplementary Material, if any, should   %% -->
<!-- %% be provided in {supplement} environment  %% -->
<!-- %% with title and short description.        %% -->
<!-- %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -->

\begin{supplement}
\stitle{Title of Supplement A}
\sdescription{Short description of Supplement A.}
\end{supplement}
\begin{supplement}
\stitle{Title of Supplement B}
\sdescription{Short description of Supplement B.}
\end{supplement}

<!-- %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -->
<!-- %%                  The Bibliography                       %% -->
<!-- %%                                                         %% -->
<!-- %%  imsart-nameyear.bst  will be used to                   %% -->
<!-- %%  create a .BBL file for submission.                     %% -->
<!-- %%                                                         %% -1`->
<!-- %%  Note that the displayed Bibliography will not          %% -->
<!-- %%  necessarily be rendered by Latex exactly as specified  %% -->
<!-- %%  in the online Instructions for Authors.                %% -->
<!-- %%                                                         %% -->
<!-- %%  MR numbers will be added by VTeX.                      %% -->
<!-- %%                                                         %% -->
<!-- %%  Use \cite{...} to cite references in text.             %% -->
<!-- %%                                                         %% -->
<!-- %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -->
