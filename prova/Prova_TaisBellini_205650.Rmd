---
title: "PPGEst - Prova EST0203 - Séries Temporais - Verão 2021"
author: "Tais Bellini"
date: "4/10/2021"
output: pdf_document
bibliography: ref.bib
header-includes:
  - \DeclareMathOperator{\EX}{\mathbb{E}}
  - \DeclareMathOperator{\Z}{\mathbb{Z}}
  - \DeclareMathOperator*{\argmax}{arg\,max}


---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Questão 1: Derivar as funções de autocorrelação:

### a) De $Y_t$ que é um ARMA(1,1) estacionário.

Seja $\{ X_t \}_{t \in Z}$ um processo estocástico ARMA(1,1) estacionário, podemos representá-lo da seguinte forma:

$$
Y_t = \phi_1 Y_{t-1} + \epsilon_t + \theta_1 \epsilon_{t-1}
$$
onde $Y_t = X_t - \mu$, $\mu = \EX{X_t}$, $\{\epsilon_t\}_{t \in Z}$ ~ $RB(0, \sigma^2)$.

A função de autocorrelação ($\rho_Y(h)$) se dá pela razão entre a função de autocovariância no lag h ($\gamma_Y(h)$) e a função de autocorrelação no lag 0 ($\gamma_Y(0)$):

$$
\rho_Y(h) = \frac{\gamma_Y(h)}{\gamma_Y(0)}
$$

Portanto, para calcular a funcão de autocorrelação, precisamos da função de autocovariância:

$$ \gamma_Y(h) = Cov(Y_t, Y_{t-h}) = Cov(Y_{t+h}, Y_t)$$
Abrindo $Y_{t+h}$, obtemos:

$$
\gamma_Y(h) = Cov[(\phi_1 Y_{t+h-1} + \epsilon_{t+h} + \theta_1 \epsilon_{t+h-1}),Y_t)]
$$
$$
= \phi_1 Cov(Y_{t+h-1},Y_t) + Cov(\epsilon_{t+h}, Y_t) + \theta_1 Cov(\epsilon_{t+h-1}, Y_t)
$$

Agora note que $Cov(Y_{t+h-1}, Y_t) = \gamma_Y(h-1)$. Assim, a função de autocorrelação $\gamma_Y(h)$ se dá por: 

$$
\gamma_Y(h) = \phi_1 \gamma_Y(h-1) + Cov(\epsilon_{t+h}, Y_t) + \theta_1 Cov(\epsilon_{t+h-1}, Y_t)
$$

Observe que, para $h > 1$, teremos os termos de covarância $Cov(\epsilon_{t+h}, Y_t)$ e $\theta_1 Cov(\epsilon_{t+h-1}, Y_t)$ zerados, pois o ruído branco do futuro não interfere em $Y_t$. Assim, neste caso, as funções de autocovariância e autocorrelacão de $Y_t$ que é um ARMA(1,1) estacionário se comportam como as de um AR(1): 

$$
\gamma_Y(h) = \phi_1\gamma_Y(h-1)
$$
$$
\rho_Y(h) = \phi_1\rho(h-1)
$$
onde $h>1$.

Já para $h \leq 1$, os termos de covariância não são zerados. Para $h=0$, temos:

\begin{equation}
\gamma_Y(0) = \phi_1 \gamma_Y(-1) + Cov(\epsilon_t, Y_t) + \theta_1Cov(\epsilon_{t-1}, Y_t)
\label{gamma0_raw}
\end{equation}

Sabemos que, quando $Y_t$ é estacionário, $\gamma_Y(-h) = \gamma_Y(h)$. Ainda, 

$$
Cov(\epsilon_t, Y_t) = Cov(\epsilon_t, (\phi_1Y_{t-1} + \epsilon_t + \theta_1\epsilon_{t-1}))
$$
$$
= \phi_1Cov(\epsilon_t, Y_{t-1}) + Cov(\epsilon_t, \epsilon_t) + \theta_1Cov(\epsilon_t, \epsilon_{t-1})
$$
onde $Cov(\epsilon_t,Y_{t-1}) = 0$, pois $Y_{t-1}$ não depende de $\epsilon$ no tempo presente. Como $\epsilon_t$ é ruído branco, $Cov(\epsilon_t, \epsilon_{t-1})=0$ e $Cov(\epsilon_t, \epsilon_t) = \sigma^2$ logo, podemos expressar

\begin{equation}
Cov(\epsilon_t, Y_t)  = \sigma^2
\label{cov_et_yt}
\end{equation}

Para determinarmos a covariância entre $\epsilon_{t-1}$ e $Y_t$, temos que:
\begin{equation}
Cov(\epsilon_{t-1}, Y_t) = \phi_1Cov(\epsilon_{t-1}, Y_{t-1}) + Cov(\epsilon_{t-1}, \epsilon_t) + \theta_1 Cov(\epsilon_{t-1}, \epsilon_{t-1})
\label{cov_et-1_yt_raw}
\end{equation}
onde $Cov(\epsilon_{t-1}, \epsilon_t) = 0$, $Cov(\epsilon_{t-1}, \epsilon_{t-1}) = \sigma^2$ e 
$$
Cov(\epsilon_{t-1}, Y_{t-1}) = Cov(\epsilon_{t-1}, (\phi_1Y_{t-2} + \epsilon_{t-1} + \theta_1\epsilon_{t-2})) = \phi_1Cov(\epsilon_{t-1}, Y_{t-2}) + Cov(\epsilon_{t-1},\epsilon_{t-1}) + \theta_1Cov(\epsilon_{t-1}, \epsilon_{t-2})
$$
no qual, pelas propriedades do processo e do ruído branco, $Cov(\epsilon_{t-1}, Y_{t-2}) = 0$, $Cov(\epsilon_{t-1}, \epsilon_{t-2}) = 0$ e $Cov(\epsilon_{t-1},\epsilon_{t-1}) = \sigma^2$. Portanto, 
\begin{equation}
Cov(\epsilon_{t-1}, Y_{t-1}) = \sigma^2
\label{cov_etmin1_ytmin1}
\end{equation}

Substituindo \ref{cov_etmin1_ytmin1} em \ref{cov_et-1_yt_raw} e considerando as propriedades do processo e de ruído branco, temos 
\begin{equation}
Cov(\epsilon_{t-1}, Y_t) = \phi_1\sigma^2 + \theta_1\sigma^2
\label{cov_et-1_yt}
\end{equation}

Finalmente, substituindo \ref{cov_et-1_yt} e \ref{cov_et_yt} em \ref{gamma0_raw}, temos:
$$
\gamma_Y(0) = \phi_1\gamma_Y(1) + \sigma^2 + \theta_1 (\phi_1 + \theta_1)\sigma^2
$$
Para $h=1$, determinamos que:

\begin{equation}
\gamma_Y(1) = \phi_1 \gamma_Y(0) + Cov(\epsilon_{t+1}, Y_t) + \theta_1 Cov(\epsilon_{t}, Y_t) = \phi_1 \gamma_Y(0) + \theta_1\sigma^2
\label{gamma1}
\end{equation}

Resolvendo $\gamma_Y(0)$ e $\gamma_Y(1)$ simultaneamente, concluímos que:

$$
\gamma_Y(0) = \frac{2\phi_1\theta_1 + 1 + \theta_1^2}{1-\phi_1^2}\sigma^2
$$
e
$$
\gamma_Y(1) =  \frac{(\phi_1^2\theta_1 + \theta_1 + \phi_1\theta_1^2 + \theta_1)\sigma^2}{1-\phi_1^2} = \frac{(1+\phi_1\theta_1)(\phi_1+\theta_1)}{1-\phi_1^2}\sigma^2
$$
Portanto, para $h \leq 1$,
$$
\rho_Y(0) = 1
$$

$$
\rho_Y(1) = \frac{\gamma_Y(1)}{\gamma_Y(0)} = \frac{(1+\phi_1\theta_1)(\phi_1+\theta_1)}{2\phi_1\theta_1 + 1 + \theta_1^2} 
$$
e para $h > 1$:
$$
\rho_Y(h) = \phi_1\rho(h-1)
$$

### b) De $Y_t^2$, onde $Y_t$ é um ARCH(2) estacionário.

Dado que $Y_t$ é um ARCH(2) estacionário, temos o seguinte modelo:

\begin{equation}
Y_t = \sigma_t\epsilon_t
\label{yt}
\end{equation}

onde $\epsilon_t \sim RB(0,1)$ e $\sigma_t^2$ é a variância condicional de $Y_t$ dadas as informações que temos até o instante $\Im_{t-1}$ que tem a seguinte estrutura:
\begin{equation}
\sigma^2 = \alpha_0 + \alpha_1Y_{t-1}^2 + \alpha_2Y_{t-2}^2
\label{sigma2}
\end{equation}

Para determinar a função de autocorrelação do $Y_t^2$, vamos elevar \ref{yt} ao quadrado em ambos os lados:

$$
Y_t^2 = \sigma_t^2\epsilon_t^2
$$

Somando e subtraindo $\sigma^2$ da equação, obtemos a seguinte estrutura:

$$
Y_t^2 = \alpha_0 + \alpha_1Y_{t-1}^2 + \alpha_2Y_{t-2}^2 + \sigma^2(\epsilon_t^2-1)
$$

Observe que $\EX{[\sigma_t^2(\epsilon_t^2-1)]} = 0$: 
$$
\EX{[\sigma_t^2(\epsilon_t^2-1)]} = \EX{\sigma_t^2}\EX{[\epsilon_t^2 - 1]}
$$
onde
$$
\EX{[\epsilon_t^2 - 1]} = \EX{\epsilon_t^2} - 1 = Var(\epsilon_t) + [\EX{\epsilon}]^2 - 1 = 1 + 0 -1 = 0
$$

Ainda, $Var(\sigma_t^2(\epsilon_t^2-1)) = Var(\epsilon_t^2-1)\EX{\sigma^2}^2$ e, sob a condição de que \ref{yt} e \ref{sigma2} têm quarto momento definido,  $Cov(\sigma_t^2(\epsilon_t^2-1), \sigma_{t-k}^2(\epsilon_{t-k}^2-1)) = 0$ para $k = 1, 2, \dots$. Assim, concluímos que $\sigma_t^2(\epsilon_t^2-1)$ é um ruído branco, e a estrutura de $Y_t^2$ se dá por: 

\begin{equation}
Y_t^2 = \alpha_0 + \alpha_1Y_{t-1}^2 + \alpha_2Y_{t-2}^2 + \varepsilon_t
\label{yt2}
\end{equation}
onde $EX{\varepsilon_t} = 0$ e $Var(\varepsilon_t) = Var(\epsilon_t^2-1)\EX{\sigma^2}^2 = \Sigma$.

Portanto, temos que $Y_t^2$ é um processo AR(2). Assim, sua função de autocorrelação ($\rho_{Y^2}$) é derivada da autocovariância ($\gamma_{Y^2}$):

$$
\gamma_{Y^2}(k) = \EX{[(Y_t^2- \mu), (Y_{t-k}^2 - \mu)]} = \EX{(Y_t^2Y_{t-k}^2)}
$$
$$
= \EX{[(\alpha_1Y_{t-1}^2 + \alpha_2Y_{t-2}^2 + \varepsilon_t)Y_{t-k}^2]}
$$
$$
= \alpha_1\EX{[Y_{t-1}^2Y_{t-k}^2]} + \alpha_2\EX{[Y_{t-2}^2Y_{t-k}^2]} + \EX{[\varepsilon_tY_{t-k}^2]}
$$
Ou seja, para $k=0$:
$$
\gamma_{Y^2}(k) = \alpha_1\gamma_{Y^2}(k-1) + \alpha_2\gamma_{Y^2}(k-2) + \Sigma
$$
E para $k>0$:
$$
\gamma_{Y^2}(k) = \alpha_1\gamma_{Y^2}(k-1) + \alpha_2\gamma_{Y^2}(k-2)
$$
Portanto, as condições iniciais se dão pela resolução das seguintes equações simultaneamente:

$$
\gamma_{Y^2}(0) = \alpha_1\gamma_{Y^2}(1) + \alpha_2\gamma_{Y^2}(2) + \Sigma
$$
$$
\gamma_{Y^2}(1) = \alpha_1\gamma_{Y^2}(0) + \alpha_2\gamma_{Y^2}(1)
$$
$$
\gamma_{Y^2}(2) = \alpha_1\gamma_{Y^2}(1) + \alpha_2\gamma_{Y^2}(0)
$$
Logo, temos:

$$
\gamma_{Y^2}(0) = \left(\frac{1-\alpha_2}{1+\alpha_2}\right)\frac{\Sigma}{(1-\alpha_2)^2-\alpha_1^2}
$$
$$
\gamma_{Y^2}(1) = \frac{\alpha_1}{1-\alpha_2}\gamma_{Y_t^2}(0) = \left(\frac{\alpha_1}{1-\alpha_2}\right)\left(\frac{1-\alpha_2}{1+\alpha_2}\right)\frac{\Sigma}{(1-\alpha_2)^2-\alpha_1^2}
$$
A partir disto, podemos derivar $\gamma_{Y^2}(k)$ para $k = 2, 3, \dots$:
$$
\gamma_{Y^2}(k) = \alpha_1\gamma_{Y^2}(k-1) + \alpha_2\gamma_{Y^2}(k-2)
$$
Finalmente, derivamos as funções de autocorrelação ($\rho_{Y_t^2}(k)$): 

$$
\rho_{Y_t^2}(1) = \frac{\gamma_{Y^2}(1)}{\gamma_{Y^2}(0)} = \frac{\alpha_1}{1-\alpha_2}
$$
$$
\rho_{Y_t^2}(2) = \alpha_1\rho_{Y_t^2}(1) + \alpha_2 = \frac{\alpha_1^2+(1-\alpha_2)\alpha_2}{1-\alpha_2}
$$
Para $k = 3, 4, \dots$:
$$
\rho_{Y_t^2}(k) = \alpha_1\rho_{Y_t^2}(k-1) + \alpha_2\rho_{Y_t^2}(k-2)
$$

## Questão 2

### a) Encontre a distribuição assintótica do estimador de máxima versossimilhança (EMV) de $\phi_1$ no modelo $y_t = \phi_0 + \phi_1y_{t-1} + \epsilon_t$ onde $\epsilon_t \sim N(0, \sigma^2)$ Use o resultado geral para os EMV's e explicite qualquer suposição adicional necessária.

Seja $\theta = (\phi_1, \dots, \phi_p, \sigma^2)$ o vetor dos parâmetros populacionais que deseja-se estimar e $\mathbf{Y} = (Y_1, \dots, Y_{T-1}, Y_T)$ um amostra observada. A função densidade de probabilidade (FDP) conjunta é denotada por:
\begin{equation}
f(Y_T, Y_{T-1} \dots, Y_1;\theta)
\label{fdp}
\end{equation}

Logo, a função de máxima verossimilhança é dada por:
\begin{equation}
L(\theta|\mathbf{Y}) = f(Y_T, Y_{T-1} \dots, Y_1;\theta)
\label{fmv}
\end{equation}

E o estimador de log-máxima verossimilhança (EMV) será o argumento que maximiza a função de máxima verossimilhança:
$$
\hat{\theta}_{EMV} = \arg \max_{\theta} l(\theta|\mathbf{Y})
$$
onde $l(\theta|\mathbf{Y}) = ln(L(\theta|\mathbf{Y}))$.

Assumindo que a derivada de $l(\theta|\mathbf{Y})$ em relação a $\theta$ existe e é contínua, determinamos o $\hat{\theta}_{EMV}$ resolvendo:
$$
\frac{\delta l(\theta|\mathbf{Y})}{\delta \theta} = 0
$$
Agora, considerando o modelo AR(p):
\begin{equation}
Y_t = \phi_0 + \phi_1Y_{t-1} + \dots + \phi_pY_{t-p} + \epsilon_t
\label{arp}
\end{equation}
onde $\epsilon_t \sim i.i.d\ N(0, \sigma^2)$, e $Y_{t-1} \dots, Y_{t-p}$ valores do passado já observados, podemos considerar que $\phi_0 + \phi_1Y_{t-1} + \dots + \phi_pY_{t-p}$ é um valor constante que está sendo somado a $\epsilon_t$ e, portanto:
$$
Y_t|\Im_{t-1} \sim N(\phi_0 + \phi_1Y_{t-1} + \dots + \phi_pY_{t-p}, \sigma^2)
$$
Ainda, sabendo que uma FDP conjunta qualquer $f(X,Y)$ pode ser decomposta em $f(Y|X)f(X)$ e conhecendo a distribuicão de $Y_t|\Im_{t-1}$ podemos reescrever \ref{fmv} como:
$$
L(\theta|\mathbf{Y}) = f(Y_t|Y_1, \dots, Y_{t-1};\theta)f(Y_{t-1}, \dots, Y_1;\theta)
$$
E assim podemos recursivamente decompor a função conjunta do segundo termo de forma a obter o produto das FDP condicionais de T-1 até 2, e ter FDP marginal de $Y_1$ no segundo termo.



Ainda, no caso de $Y_1$, vamos retomar o fato de que, para um processo estacionário AR(1):
$$
\EX{Y_1} = \mu = \frac{\phi_0}{1-\phi_1}
$$
$$
Var(Y_1) = \frac{\sigma^2}{1-\phi_1^2}
$$
Portanto, temos que $Y_1 \sim N(\frac{\phi_0}{1-\phi_1}, \frac{\sigma^2}{1-\phi_1^2})$.

Olhando especificamente para o modelo AR(1):
\begin{equation}
Y_t = \phi_0 + \phi_1Y_{t-1} + \epsilon_t
\label{ar1}
\end{equation}

A decomposição da função de máxima verossimilhança é feita da seguinte forma:

$$
L(\theta|\mathbf{Y}) = f(Y_t|Y_{t-1};\theta)f(Y_{t-1};\theta)
$$
https://faculty.washington.edu/ezivot/econ584/notes/armaestimation.pdf
http://www.science.unitn.it/AnalisiInfoTLC/SSP/SSP14_15/lectures/lecture15.pdf
https://math.unice.fr/~frapetti/CorsoP/Chapitre_4_IMEA_1.pdf
http://www-stat.wharton.upenn.edu/~stine/stat910/lectures/12_est_arma.pdf
http://course.sdu.edu.cn/G2S/eWebEditor/uploadfile/20140110134920017.pdf

## Questão 3

### a) Mostre que estimar um VAR(p) padrão de dimensão n (não estrutural) por MV é equivalente a estimar por MQO linha por linha (das n linhas) do modelo.

Dado um modelo multivariado:

\begin{equation}
\mathbf{y_t} = \mathbf{c} + \Phi_1\mathbf{y_{t-1}} + \dots + \Phi_p\mathbf{y_{t-p}} + \mathbf{\epsilon_t}
\label{eq:var}
\end{equation}

onde $\mathbf{y_t} = \begin{bmatrix} y_{1t} \\ \vdots \\ y_{nt} \end{bmatrix}$ é um vetor contendo $\mathbf{y_t}$ para cada uma das n variáveis, $\mathbf{c} = \begin{bmatrix} c_1 \\ \vdots \\ c_n \end{bmatrix}$ é um vetor de constantes, $\Phi_j = \begin{bmatrix} \phi_{11}^{(j)} & \dots & \phi_{1n}^{(j)} \\ \vdots \\ \phi_{n1}^{(j)} & \dots & \phi_{nn}^{(j)} \end{bmatrix}$ é uma matriz com os parâmetros para todas as variáveis com aquela defasagem e $\mathbf{\epsilon_t} \sim N(\mathbf{0}, \Omega)$ é um vetor de ruídos. 

Dessa forma, temos um sistema de equações no qual a j-ésima linha se dá por: 

\begin{equation}
\begin{aligned}
y_{jt} = c_j & + \phi_{j1}^{(1)}y_{1,t-1} + \dots + \phi_{jn}^{(1)}y_{n,t-1} +
             & + \phi_{j1}^{(2)}y_{1,t-2} + \dots + \phi_{jn}^{(2)}y_{n,t-2} + \dots 
             & + \phi_{j1}^{(p)}y_{1,t-p} + \dots + \phi_{jn}^{(p)}y_{n,t-p} + \epsilon_{jt} 
\label{eq:varj}
\end{aligned}
\end{equation}

Dado o sistema da equação \ref{eq:var}, consideramos o vetor $\mathbf{c_t} = \mathbf{c} + \Phi_1\mathbf{y_{t-1}} + \dots + \Phi_p\mathbf{y_{t-p}}$, onde $y_t|\Im_{t-1} \sim NM(\mathbf{c_t}, \Omega)$, e o vetor $\mathbf{\theta}  = (\mathbf{c}, \Phi_1, \dots, \Phi_p, \Omega)$. 
O estimador de máxima verossimilhança desde sistema, como visto em aula, é definido da seguinte forma:

\begin{equation}
L(\mathbf{\theta}) = \prod_{t=1}^{T}f(\mathbf{y_t}|\Im_{t-1};\theta) = (2\pi)^{-Tn/2}|\Omega^{-1}|exp\{-\frac{1}{2}\sum_{t-1}^T(\mathbf{y_t}-\mathbf{c_t})^\intercal\Omenga^{-1}(\mathbf{y_t}-\mathbf{c_t})}
\label{eq:lvar}
\end{equation}

Denotamos, ainda: 
$$
\mathbf{c_t} = \Pi^\intercal\mathbf{x_t}
$$
onde $\Pi^\intercal = (\mathbf{c}, \Phi_1, \dots, \Phi_p)$ e 
$$
\mathbf{x_t} = \begin{bmatrix} 1 \\ \mathbf{y_{t-1}} \\ \vdots \\ \mathbf{y_{t-p}} \end{bmatrix}
$$
Fazendo a derivação em relação a $\Pi$ do logaritmo natural de \ref{eq:lvar} para obter o estimador de MV dos parâmetros do modelo, obtemos o seguinte resultado:
$$
\hat{\Pi}^\intercal = \frac{\left[\sum_{t=1}^T\mathbf{y_t}\mathbf{x_t}^\intercal\right]}{\left[\sum_{t=1}^T\mathbf{x_t}\mathbf{x_t}^\intercal\right]}
$$
Note que a j-ésima linha de $\hat{\Pi}^\intercal$ é definida por: 

\begin{equation}
\hat{\pi_j}^\intercal = \frac{\left[\sum_{t=1}^Ty_{jt}\mathbf{x_t}^\intercal\right]}{\left[\sum_{t=1}^T\mathbf{x_t}\mathbf{x_t}^\intercal\right]}
\label{eq:varmv}
\end{equation}

Ou, em sua forma vetorial: 

\begin{equation}
\hat{\pi_j}^\intercal = \frac{X^\intercal y_j}{(X^\intercal X)}
\label{eq:varmvvet}
\end{equation}

onde 
$$
y_{j} = (y_{j1}, \dots, y_{jT})^\intercal
$$
$$
X = \begin{bmatrix} 
      1 & y_{1,0} & \dots & y_{n,0} \\
      \vdots & & & \vdots \\
      1 & y_{1,t-1} & \dots & y_{n,t-1}
      \end{bmatrix}
$$

Agora, observe a equação da j-ésima linha em \ref{eq:varj}. Sua forma vetorial se dá por: 

$$
y_{j} = X\beta_j + \epsilon_j
$$
onde 
$$
y_{j} = (y_{j1}, \dots, y_{jT})^\intercal
$$
$$
X = \begin{bmatrix} 
      1 & y_{1,0} & \dots & y_{n,0} \\
      \vdots & & & \vdots \\
      1 & y_{1,t-1} & \dots & y_{n,t-1}
      \end{bmatrix}
$$
$$
\beta_j = (c_1, \phi_{j1}, \dots, \phi_{jn})^\intercal
$$
e
$$
\epsilon_j = (\epsilon_{j1}, \dots, \epsilon_{jT})^\intercal
$$

Se obtivermos o estimador de mínimos quadrados (OLS) para estimar $\beta_j$ da equação acima, temos:
$$
\hat{\beta_j} = \frac{X^\intercal y_j}{(X^\intercal X)}
$$
Que é exatamente equivalente a equação \ref{eq:varmvvet}.

Para a estimação de $\Omega$, temos:

$$
\hat{\Omega} = \frac{1}{T} \sum_{t=1}^{T}\mathbf{\hat{\epsilon_t}^\intercal\hat{\epsilon_t}}
$$
onde
$$
\mathbf{\hat{\epsilon_t}} = \begin{bmatrix} \hat{\epsilon_{1t}} \\ \vdots \\ \hat{\epsilon_{nt}} \end{bmatrix}
$$
e $\hat{\epsilon_{jt}}$ são os resíduos do ajuste de mínimos quadrados da j-ésima equação.

### b) Com uma série temporal vetorial multivariada a sua escolha (mas com dimensão mínima de 3), estime um VAR ou VEC (o que for mais adequado – faça testes),mostrando os resultados e os interpretando.

Para esta questão, será utilizado um banco de dados utilizado em @Hoseinzade2019 que contém várias categorias de indicadores do mercado financeiro, como preços de commodities, índices importantes de mercados em todo o mundo, preços das principais empresas no mercado dos EUA e taxas de títulos do tesouro. Os dados foram observados diariamente de 2010 até 2017. Foram selecionadas 3 variáveis do banco de dados: Preço final do índice S&P (Close), que contém quinhentos ativos cotados nas bolsas de NYSE ou NASDAQ, Índice do Dólar (Dollar.index) e taxa de títulos do Tesouro dos Estados Unidos(DTB4WK).

```{r}
library(urca)
library(tsDyn)
library(vars)
library(tseries)
data = read.csv2("Processed_S&P.csv", sep = ",")
data = data[-1,]

dollar = ts(as.numeric(data$Dollar.index))
dtb4wk = ts(as.numeric(data$DTB4WK))
close = ts(as.numeric(data$Close))

ds = cbind(close, dollar, dtb4wk)
ds = scale(ds[-1,])

plot(ts(ds), main = "Plot de cada série do banco de dados",  yax.flip = F)
```

Para uma análise de séries temporais multivariadas, temos o modelo VAR, cuja fórmula geral VAR(p) tem a seguinte estrutura:

$$
Y_t = \mathbf{\mu} + A_1\mathbf{Y_{t-1}} + \dots + A_p\mathbf{Y_{t-p}} + \mathbf{\epsilon_t}
$$
onde $\mu$ é o vetor de médias da série, $A_i$ são as matrizes dos coeficientes para cada lag e $\epsilon_t$ é vetor de erros com distribuição normal multivariada com média zero. 

Para utilizá-lo, é necessário avaliar se as séries a serem trabalhadas são estacionárias:

```{r}
par(mfrow=c(1,3))
acf(close, main = "ACF Close")
acf(dollar, main = "ACF Dollar Index")
acf(dtb4wk, main = "ACF DTB4WK")

par(mfrow=c(1,3))
pacf(close)
pacf(dollar)
pacf(dtb4wk)

```

```{r, warning = F}
c = adf.test(close)
d = adf.test(dollar)
a = adf.test(dtb4wk)

adf_pvalues = c(c$p.value,  d$p.value, a$p.value)
names(adf_pvalues) = c("Close", "Dollar Index", "DTB4WK")
paste("P-Value do teste ADF")
adf_pvalues
```

Nota-se que apenas a variável do Índice do Dólar é estacionária. 

Agora, precisamos verificar a existência de cointegração entre as variáveis. Faremos isto através do método dos autovalores de Johansen, que permite o teste para mais do que duas variáveis ao mesmo tempo. Para isto, temos o modelo VEC, que faz a diferenciação das séries obtendo a seguinte forma:

$$
\Delta \mathbf{Y_t} = \mathbf{\mu} + A\mathbf{Y_{t-1}} + \Gamma_1\Delta \mathbf{Y_{t-1}} + \dots + \Gamma_p\Delta \mathbf{Y_{t-p}} + \mathbf{\epsilon_t}
$$
onde $\Delta \mathbf{Y_{t}} = \mathbf{Y_t} - \mathbf{Y_{t-1}}$ é o operador de diferenciação, $A$ é a matriz de coeficientes para o primeiro lag e $\Gamma_i$ as matrizes dos coeficientes para cada lag. 

O teste de Johansen verifica sequencialmente se o posto da matriz $A$ é 0, 1, 2, ... até o número de séries sendo avaliadas menos um. Se o posto da matriz A = 0, não há co-integração.

Para realizar o teste, precisamos escolher o lag ótimo:

```{r}
library(vars)
lagselect = VARselect(ds, lag.max = 20, type = "const")
lagselect$selection
```

O lag ótimo obtivo para o VAR é 5, portanto, para o VEC, utilizamos 4:

```{r}
library(urca)
# K = lag -1
ctest1e <- ca.jo(ds, type = "eigen", ecdet = "const", K = 4)
summary(ctest1e)
```

Rejeitamos a hipótese de que o número de cointegrações seja zero e não rejeitamos a hipótese de que o número seja menor ou igual a 1.

Portanto, iremos construir um modelo VEC com $r=1$:
```{r, warning = F}
vec_model <- VECM(ds, 4, r = 1, estim =("2OLS"))
summary(vec_model)
```

Observa-se pelo vetor de cointegração que o preço final do índice S&P tem uma relação negativa pequena com o índice do Dólar e um relação negativa maior com os títulos do Tesouro dos Estados Unidos. Ainda, percebe-se que um efeito tardio do preço final do índice nele mesmo, pois apenas o seu valor no lag 3 é significativo para o preço final do índice. Os valores das duas demais variáveis têm uma relação de efeito significativa com todos os seus lags. 

## Questão 4: Utilizando uma série temporal a sua escolha (mas com pelo menos 10 séries temporais como covariáveis e pelo menos 3 lags de cada uma delas, ou seja, pelo menos 30 covariáveis), implemente, para estimação e seleção, todos métodos vistos em aula sobre machine/statistical learning: ridge, lasso, adalasso, tress (baging, random forest e boosting). Mostre os resultados e os interprete.

Para a implementacão desta questão foi utilizado o mesmo banco de dados da questão 3 e foram selecionados 10 ativos financeiros: Oil, Gold, JPM, AMZN, AAPL, GE, DJI, NYSE, EUR, Dollar.index e o preço final (Close) do índice S&P.

```{r, warning=F, echo=F, include=F}
library(ggplot2)
library(tidyr)
```


```{r}
data = read.csv2("Processed_S&P.csv", sep = ",")

oil = ts(as.numeric(data$Oil))
gold = ts(as.numeric(data$Gold))
jpm = ts(as.numeric(data$JPM))
close = ts(as.numeric(data$Close))
amzn = ts(as.numeric(data$AMZN))
ge = ts(as.numeric(data$GE))
dji = ts(as.numeric(data$DJI))
nyse = ts(as.numeric(data$NYSE))
eur = ts(as.numeric(data$EUR))
aapl = ts(as.numeric(data$AAPL))
dollar = ts(as.numeric(data$Dollar.index))

df = data.frame(close, oil, gold, jpm, amzn, aapl, ge, dji, nyse, eur, dollar)
lines = nrow(df)

# Adicionar os valores de lag 
df = within(df, {
  close1 = c(rep(NA,1), close[-seq(from = lines, by = -1, length.out = 1)])
  close2 = c(rep(NA,2), close[-seq(from = lines, by = -1, length.out = 2)])
  close3 = c(rep(NA,3), close[-seq(from = lines, by = -1, length.out = 3)])
  oil1 = c(rep(NA,1), oil[-seq(from = lines, by = -1, length.out = 1)])
  oil2 = c(rep(NA,2), oil[-seq(from = lines, by = -1, length.out = 2)])
  oil3 = c(rep(NA,3), oil[-seq(from = lines, by = -1, length.out = 3)])
  gold1 = c(rep(NA,1), gold[-seq(from = lines, by = -1, length.out = 1)])
  gold2 = c(rep(NA,2), gold[-seq(from = lines, by = -1, length.out = 2)])
  gold3 = c(rep(NA,3), gold[-seq(from = lines, by = -1, length.out = 3)])
  jpm1 = c(rep(NA,1), jpm[-seq(from = lines, by = -1, length.out = 1)])
  jpm2 = c(rep(NA,2), jpm[-seq(from = lines, by = -1, length.out = 2)])
  jpm3 = c(rep(NA,3), jpm[-seq(from = lines, by = -1, length.out = 3)])
  amzn1 = c(rep(NA,1), amzn[-seq(from = lines, by = -1, length.out = 1)])
  amzn2 = c(rep(NA,2), amzn[-seq(from = lines, by = -1, length.out = 2)])
  amzn3 = c(rep(NA,3), amzn[-seq(from = lines, by = -1, length.out = 3)])
  aapl1 = c(rep(NA,1), aapl[-seq(from = lines, by = -1, length.out = 1)])
  aapl2 = c(rep(NA,2), aapl[-seq(from = lines, by = -1, length.out = 2)])
  aapl3 = c(rep(NA,3), aapl[-seq(from = lines, by = -1, length.out = 3)])
  ge1 = c(rep(NA,1), ge[-seq(from = lines, by = -1, length.out = 1)])
  ge2 = c(rep(NA,2), ge[-seq(from = lines, by = -1, length.out = 2)])
  ge3 = c(rep(NA,3), ge[-seq(from = lines, by = -1, length.out = 3)])
  dji1 = c(rep(NA,1), dji[-seq(from = lines, by = -1, length.out = 1)])
  dji2 = c(rep(NA,2), dji[-seq(from = lines, by = -1, length.out = 2)])
  dji3 = c(rep(NA,3), dji[-seq(from = lines, by = -1, length.out = 3)])
  nyse1 = c(rep(NA,1), nyse[-seq(from = lines, by = -1, length.out = 1)])
  nyse2 = c(rep(NA,2), nyse[-seq(from = lines, by = -1, length.out = 2)])
  nyse3 = c(rep(NA,3), nyse[-seq(from = lines, by = -1, length.out = 3)])
  dollar1 = c(rep(NA,1), dollar[-seq(from = lines, by = -1, length.out = 1)])
  dollar2 = c(rep(NA,2), dollar[-seq(from = lines, by = -1, length.out = 2)])
  dollar3 = c(rep(NA,3), dollar[-seq(from = lines, by = -1, length.out = 3)])
  eur1 = c(rep(NA,1), eur[-seq(from = lines, by = -1, length.out = 1)])
  eur2 = c(rep(NA,2), eur[-seq(from = lines, by = -1, length.out = 2)])
  eur3 = c(rep(NA,3), eur[-seq(from = lines, by = -1, length.out = 3)])
})

df = df[-c(1:4),]
```

#### Ridge

A regressão Ridge é uma regressão de mínimos quadrados ordinários que realiza o _shrinkage_ dos coeficientes a partir de uma penalização na soma dos seus quadrados impondo que esta soma seja menos que um valor definido $t$. Assim, temos que: 

$$
\hat{\beta}_{ridge} = \arg \min_{\beta_0, \dots, \beta_k}  \Bigg\{\sum_{i=1}^n\left(y_i - \beta_0 - \sum_{j=1}^k\beta_j x_{ij}\right) + \lambda\sum_{j=1}^k\beta_j^2\Bigg\}
$$

O resultado são coeficientes com valores menores do que uma regressão de mínimos quadrados tradicional (OLS).

Vamos determinar a variável resposta e centralizá-la, já que o algoritmo de regressão Ridge parte do pressuposto que os preditores estão padronizados e a variável resposta centralizada. 

```{r}
# variavel resposta:
y = df$close
y = y
#covariaveis
X = as.matrix(df[,-1])
X = X
```


Vamos observar como fica o ajuste em uma regressão de mínimos quadrados tradicional:
```{r}
library(knitr)
library(glmnet)
#OLS
fit.OLS = glmnet(y=y,x=X, lambda=0, alpha = 0)
B.hat.OLS = as.vector(fit.OLS$beta)
B0.hat.OLS = fit.OLS$a0
B.hat.OLS = c(B0.hat.OLS, B.hat.OLS)
names(B.hat.OLS) = c("Intercept", rownames(fit.OLS$beta))
kable(B.hat.OLS)
```


Executando a regressão ridge com lambdas 0.05 e 0.1:

``` {r, warning = F}
lambda_005 = 0.05
lambda_01 = 0.1

#Ridge Regression - alpha = 0 
fit.RR.005 = glmnet(y=y,x=X, lambda=lambda_005, alpha = 0)
fit.RR.01 = glmnet(y=y,x=X, lambda=lambda_01, alpha = 0)

#Coeficientes do RR
B.hat.RR.005 = as.vector(fit.RR.005$beta)
B.hat.RR.01 = as.vector(fit.RR.01$beta)

#Intercepto do RR
B0.hat.RR.005 = fit.RR.005$a0
B0.hat.RR.01 = fit.RR.01$a0

B.hat.RR.005 = c(B0.hat.RR.005, B.hat.RR.005)
B.hat.RR.01 = c(B0.hat.RR.01, B.hat.RR.01)
names(B.hat.RR.005) = names(B.hat.RR.005) = names(B.hat.OLS)

lambdas = cbind(0, lambda_005, lambda_01)
coefs = cbind(B.hat.OLS, B.hat.RR.005, B.hat.RR.01)
rownames(lambdas) = "Lambda value"
kable(rbind(coefs, lambdas))

sums = cbind(sum(abs(B.hat.OLS)), sum(abs(B.hat.RR.005)), sum(abs(B.hat.RR.01)))
colnames(sums) = c("Sum of coefs OLS", "Sum of coefs lambda = 0.05", "Sum of coefs lambda = 0.1")
kable(sums)
```

Conseguimos observar que as regressões executadas com $\lambda > 0$ reduziram os coeficientes. O valor dos coeficientes diminui a medida que o $\lambda$ aumenta, como observado no gráfico abaixo:


```{r}
lambdas_to_try <- 10^seq(-2, 5, length.out = 100)
res <- glmnet(X, y, alpha = 0, lambda = lambdas_to_try, standardize = T)
plot(res, xvar = "lambda")
```
Agora, vamos avaliar o ajuste do modelo obtido de acordo com cada lambda escolhido:

```{r, warning = F}
library(tidyverse)
y_hat_ols = predict(fit.OLS, X)
ssr_ols = t(y - y_hat_ols) %*% (y - y_hat_ols)

y_hat_005 = predict(fit.RR.005, X)
ssr_005 = t(y - y_hat_005) %*% (y - y_hat_005)

y_hat_01 = predict(fit.RR.01, X)
ssr_01 = t(y - y_hat_01) %*% (y - y_hat_01)


y_hat = ts(cbind(y,  y_hat_ols, y_hat_005, y_hat_01))
colnames(y_hat) = c("real", "ols", "005", "01")
df_y <- data.frame(y_hat)
df_y$date <- as.numeric(row.names(df_y))


df_gathered = df_y %>%
  gather(key = "variable", value = "value", -date)

ggplot(df_gathered, aes(x = date, y = value)) + 
  geom_line(aes(color = variable), size = 1) +
  theme_minimal()

ssr = cbind(ssr_ols, ssr_005, ssr_01)
colnames(ssr) = c("ols", "005", "01")
rownames(ssr) = "SSR"
kable(ssr)
```

Observa-se que os métodos com penalização resultam em menos resíduos e ajusta melhor o modelo. 

#### Lasso

A regressão utilizando Lasso é muito parecida com a regressão Ridge, mas neste caso penaliza-se a soma dos valores absolutos dos coeficientes, ao invés dos quadrados:

$$
\hat{\beta}_{lasso} = \arg \min_{\beta_0, \dots, \beta_k}  \Bigg\{\sum_{i=1}^n\left(y_i - \beta_0 - \sum_{j=1}^k\beta_j x_{ij}\right) + \lambda\sum_{j=1}^k|\beta_j|\Bigg\}
$$

Como resultado, temos coeficientes que são totalmente zerados, permitindo uma seleção de variáveis. 

Testando com os valores de $\lambda$ fixado em 0.1 e escolhido por cross-validated:
```{r}
lasso_cv <- cv.glmnet(X, y, alpha = 1, lambda = lambdas_to_try, standardize = F, nfolds = 10)
lambda <- lasso_cv$lambda.min

fit.LASSO.CV = glmnet(y=y,x=X, lambda = lambda, alpha = 1)
fit.LASSO.Fixed01 = glmnet(y=y,x=X, lambda = 0.1, alpha = 1)


#Coeficientes do LASSO
B.hat.LASSO.CV = as.vector(fit.LASSO.CV$beta)
B.hat.LASSO.Fixed01 = as.vector(fit.LASSO.Fixed01$beta)


#Intercepto do LASSO
B0.hat.LASSO.CV = fit.LASSO.CV$a0
B0.hat.LASSO.Fixed01 = fit.LASSO.Fixed01$a0

B.hat.LASSO.CV = c(B0.hat.LASSO.CV, B.hat.LASSO.CV)
B.hat.LASSO.Fixed01 = c(B0.hat.LASSO.Fixed01, B.hat.LASSO.Fixed01)
names(B.hat.LASSO.CV) = names(B.hat.LASSO.Fixed01) = names(B.hat.OLS)

lambdas = cbind(lambda, 0.1)
coefs = cbind(B.hat.LASSO.CV, B.hat.LASSO.Fixed01)
rownames(lambdas) = "Lambda value"
kable(rbind(coefs, lambdas))
```

Observa-se que o método de cross-validation escolheu o $\lambda = 0.01$ e, em comparação com o $\lambda = 0.1$ zerou menos coeficientes, o que é esperado, pois quanto maior o $\lambda$, mais variáveis serão zeradas, como indicado no gráfico abaixo. 

```{r}
res <- glmnet(X, y, alpha = 1, lambda = lambdas_to_try, standardize = FALSE)
plot(res, xvar = "lambda")
legend("bottomright", lwd = 1, col = 1:6, legend = colnames(X), cex = .7)
```

As variáveis NYSE e DJI, que representam outros índices da bolsa de valores, e seus lags se mostraram significativos para o preço final do índice S&P. Ainda, grandes empresas como Amazon, General Eletric e Apple e seus lags possuem influência no preço final. Com o $lambda = 0.01$, o preço do ouro foi desconsiderado, assim como o índice do dólar com defasagem = 3. Com o $\lambda = 0.1$, mais variáveis foram removidas do modelo, como esperado, sendo algumas delas as variáveis dos índices NYSE e DJI e das grandes empresas com 3 defasagens, o banco JP Morgan e o preço do petróleo. 

```{r}
y_hat_cv = predict(fit.LASSO.CV, X)
ssr_cv = t(y - y_hat_cv) %*% (y - y_hat_cv)

y_hat_01 = predict(fit.LASSO.Fixed01, X)
ssr_01 = t(y - y_hat_01) %*% (y - y_hat_01)

y_hat = ts(cbind(y,  y_hat_cv, y_hat_01))
colnames(y_hat) = c("real", "cv", "fixed 0.1")
df_y <- data.frame(y_hat)
df_y$date <- as.numeric(row.names(df_y))

df_gathered = df_y %>%
  gather(key = "variable", value = "value", -date)

ggplot(df_gathered, aes(x = date, y = value)) + 
  geom_line(aes(color = variable), size = 1) +
  theme_minimal()

ssr = cbind(ssr_cv, ssr_01)
colnames(ssr) = c("cv", "fixed 0.1")
rownames(ssr) = "SSR"
kable(ssr)

```

O modelo com o $\lambda = 0.01$ obteve um melhor ajuste na predição, já que utiliza mais variáveis. Porém, observamos no gráfico que ambos fazem um bom ajuste.


#### AdaLasso

O método de AdaLasso introduz a ideia de adicionar pesos na penalização, de forma que valores altos de $\beta$ estimados recebem pesos menores e, consequentemente, menores penalizações, em oposição a valores baixos de $\beta$. 

$$
\hat{\beta}_{adaLASSO} = \arg \min_{\beta_0, \dots, \beta_k} \Bigg\{\sum_{i=1}^n\left(y_i - \beta_0 - \sum_{j=1}^k \beta_j x_{ij} \right) + \lambda \sum_{j=1}^k\hat{\omega_j}|\beta_j|\Bigg\}
$$
onde $\hat{\mathbf{\omega}}=|\hat{\beta}|^{-\gamma}$, $\gamma >0$ e $\beta$ é um estimador consistente, como os obtidos por Ridge ou OLS.

Implementando AdaLasso com $\lambda \simeq 0.01$ e $\gamma = 0.5$:
```{r}
#Calculo dos pesos utilizando os coeficientes do LASSO como Primeiro Estagio
n = nrow(X)
w = (abs(B.hat.LASSO.CV)+(n)^(-1/2))^(-1)

# Segundo Estagio do adaLASSO
fit.adaLASSO = glmnet(y=y,x=X, lambda = lambda, alpha = 1, penalty.factor = w)

#Coeficentes do adaLASSO
B.hat.adaLASSO = as.vector(fit.adaLASSO$beta)

#Intercepto do adaLASSO
B0.hat.adaLASSO = fit.adaLASSO$a0

B.hat.adaLASSO = c(B0.hat.adaLASSO, B.hat.adaLASSO)
names(B.hat.adaLASSO) = names(B.hat.OLS)

lasso_coefs = cbind(B.hat.adaLASSO, B.hat.LASSO.CV)
colnames(lasso_coefs) = c("adaLASSO", "LASSO")
lasso_coefs
```

Observa-se que com este método, a variável que indica o preço final do índice S&P com lag = 1 e o valor do banco JP Morgan foi removida, e os valores de ouro e índice do dólar com 3 defasagens foram adicionados, em comparação com o método de regressão LASSO. 

```{r}
y_hat_lasso = predict(fit.LASSO.CV, X)
ssr_lasso = t(y - y_hat_lasso) %*% (y - y_hat_lasso)

y_hat_adalasso = predict(fit.adaLASSO, X)
ssr_adalasso = t(y - y_hat_adalasso) %*% (y - y_hat_adalasso)

y_hat = ts(cbind(y,  y_hat_lasso, y_hat_adalasso))
colnames(y_hat) = c("real", "lasso", "adaLasso")
df_y <- data.frame(y_hat)
df_y$date <- as.numeric(row.names(df_y))

df_gathered = df_y %>%
  gather(key = "variable", value = "value", -date)

ggplot(df_gathered, aes(x = date, y = value)) + 
  geom_line(aes(color = variable), size = 1) +
  theme_minimal()

ssr = cbind(ssr_lasso, ssr_adalasso)
colnames(ssr) = c("lasso", "adalasso")
rownames(ssr) = "SSR"
kable(ssr)
```

Para este caso, a regressão LASSO se mostrou levemente superior.

#### Trees

O método de árvores de decisão consiste em recursivamente encontrar a variável que será usada para particionar os dados remanescentes de forma que a soma do quadrado dos erros entre o valor real da variável resposta e a constante predita naquele grupo é minimizado.

Vamos utilizar o método _rpart_ para gerar uma árvore de regressão que faça a previsão do preço final do índice S&P. 

```{r}
library(rpart)

test = df[c(1:20),]
train = df[-c(1:20),]

sp.close <- rpart(
  formula = close ~ .,
  data    = train,
  method  = "anova",
  model = F,
  control = list(cp = 0)
)
close.hat = predict(sp.close, newdata = test)
close.test = test$close

close.comp = data.frame(date = rep(1:20), predicted = close.hat, actual = close.test)

df_gathered = close.comp %>%
  gather(key = "variable", value = "value", -date)

ggplot(df_gathered, aes(x = date, y = value)) + 
  geom_line(aes(color = variable), size = 1) +
  theme_minimal() + ggtitle("Predicted vs. Actual")

paste("Média da soma dos erros quadráticos")
mean((close.hat - close.test)^2)
```
Nota-se que o método acompanha a tendência dos valores do preço final, com um erro quadrático médio de 105.35.

Vamos ver quais foram as variáveis mais importantes para a árvore de decisão:
```{r, warning=F}
library(vip)
vip(sp.close, num_features = 40, bar = FALSE)
```
Os valores passados do preço do índice S&P foram os mais significativos para a tomada de decisão da árvore, em seguida os preços do banco JP Morgan (com zero, 2 e 3 defasagens), e das grandes empresas americanas Amazon, Apple e General Eletrics.

A árvore construída possui mais de 1000 splits. Portanto, vamos identificar se é possível gerar uma árvore mais otimizada:
```{r}
library(rpart.plot)

plotcp(sp.close)
abline(v = 8, lty = "dashed")
```
Notamos que a partir de 8 splits, começamos a ter uma estabilidade nos erros de predição. Portanto, vamos tentar fazer a poda da árvore para apenas 8 splits:

```{r}
cp.table = as.data.frame(sp.close$cptable)
cp = cp.table[cp.table$nsplit ==10,]$CP
pfit = prune(sp.close, cp=cp) # from cptable   
rpart.plot(pfit)

close.hat = predict(pfit, newdata = test)
close.test = test$close

close.comp = data.frame(date = rep(1:20), predicted = close.hat, actual = close.test)

df_gathered = close.comp %>%
  gather(key = "variable", value = "value", -date)

ggplot(df_gathered, aes(x = date, y = value)) + 
  geom_line(aes(color = variable), size = 1) +
  theme_minimal() + ggtitle("Predicted vs. Actual")

paste("Média da soma dos erros quadráticos")
mean((close.hat - close.test)^2)
```

Perecebe-se um erro médio maior, em comparação com a árvore sem poda, o que é esperado. Porém o ajuste ficou muito longe do ideal e apenas as variáveis de defasagem do preço foram utilizadas. Vamos tentar uma abordagem intermediária, com menos splits que a árvore original, mas mais do que apenas 8:

```{r}
cp.table = as.data.frame(sp.close$cptable)
cp = cp.table[cp.table$nsplit ==50,]$CP
pfit = prune(sp.close, cp=cp) # from cptable   
rpart.plot(pfit)

close.hat = predict(pfit, newdata = test)
close.test = test$close

close.comp = data.frame(date = rep(1:20), predicted = close.hat, actual = close.test)

df_gathered = close.comp %>%
  gather(key = "variable", value = "value", -date)

ggplot(df_gathered, aes(x = date, y = value)) + 
  geom_line(aes(color = variable), size = 1) +
  theme_minimal() + ggtitle("Predicted vs. Actual")

paste("Média da soma dos erros quadráticos")
mean((close.hat - close.test)^2)

```
Com 50 splits temos uma árvore maior, que utiliza mais algumas variáveis para a tomada de decisão, como o preço dos índices NYSE e DJI. O erro médio ainda é maior do que da árvore original, mas menos que a metade do que com 8 splits. 

##### Random Forest e Bagging

Um problema do método de árvore de regressão sem menhuma otimização e utilizando todas as variáveis é o _over-fitting_. Para contornar isto, usa-se o método de Random Forest, no qual selecionamos um número de variáveis que devem ser utilizadas e que serão aleatoriamente escolhidas para serem consideradas no momento de realizar o split. Este algoritmo treina diversas árvores a partir de amostras Bootstrap do banco de dados de treinamento, realizando o procedimento de escolha aleatória das variáveis a serem utilizadas e realizando a predição baseada na média dos resultados obtidos em todas as árvores geradas. Dessa forma, evitamos o uso de variáveis altamente correlacionadas e o _over-fitting_. É comum utilizar a raiz quadrada do número de variáveis preditoras do modelo. No nosso caso, temos 43 preditores, então vamos utilizar 7 variáveis:

```{r}
library(randomForest)
rf = randomForest(close ~ .,
  data = train,
  mtry = 7,
  importance = T,
)

rf.hat = predict(rf, newdata = test)
rf.test = test$close

close.comp = data.frame(date = rep(1:20), predicted = rf.hat, actual = rf.test)

df_gathered = close.comp %>%
  gather(key = "variable", value = "value", -date)

ggplot(df_gathered, aes(x = date, y = value)) + 
  geom_line(aes(color = variable), size = 1) +
  theme_minimal() + ggtitle("Predicted vs. Actual")

paste("Média da soma dos erros quadráticos")
mean((rf.hat - rf.test)^2)

paste("Número de árvores")
rf$ntree

plot(rf, main = "Erro médio vs. número de árvores")
```
Observa-se que com apenas 7 variáveis, o erro quadrático médio fica muito elevado. Ainda, foram geradas 500 árvores para fazer a predição, mas pelo gráfico, percebe-se que a partir de 100 o erro se estabiliza. Vamos tentar com mais variáveis e menos árvores: 

```{r}
rf = randomForest(close ~ .,
  data = train,
  mtry = 10,
  importance = T,
  ntree = 100
)

rf.hat = predict(rf, newdata = test)
rf.test = test$close

close.comp = data.frame(date = rep(1:20), predicted = rf.hat, actual = rf.test)

df_gathered = close.comp %>%
  gather(key = "variable", value = "value", -date)

ggplot(df_gathered, aes(x = date, y = value)) + 
  geom_line(aes(color = variable), size = 1) +
  theme_minimal() + ggtitle("Predicted vs. Actual")

paste("Média da soma dos erros quadráticos")
mean((rf.hat - rf.test)^2)

paste("Número de árvores")
rf$ntree

plot(rf, main = "Erro médio vs. número de árvores")
```

Observamos que com 10 variáveis, já reduzimos bastante o erro quadrático médio da previsão. 

```{r}
library(vip)
vip(rf, num_features = 40, bar = FALSE)
```
Percebe-se ainda que o preço final do índice S&P com 1, 2 e 3 defasagens tem bastante importância para a árvore de decisão. Em seguida, o banco JP Morgan e os índices NYSE e DJI. 

O método de Bagging segue a mesma lógica do Random Forest, onde diversas árvores são geradas para a previsão e o resultado final é a média das previsões de todas elas. A diferença é que no Bagging não limitamos o número de variáveis para realizar o split das amostras Boostrap.

```{r}
q = (ncol(train)-1)
bag = randomForest(close ~ .,
  data = train,
  mtry = q,
  importance = T,
  ntree = 100
)

bag.hat = predict(bag, newdata = test)
bag.test = test$close

close.comp = data.frame(date = rep(1:20), predicted = bag.hat, actual = bag.test)

df_gathered = close.comp %>%
  gather(key = "variable", value = "value", -date)

ggplot(df_gathered, aes(x = date, y = value)) + 
  geom_line(aes(color = variable), size = 1) +
  theme_minimal() + ggtitle("Predicted vs. Actual")

paste("Média da soma dos erros quadráticos")
mean((bag.hat - bag.test)^2)
```

Percebe-se que o erro quadrático médio da previsão com este método é inferior a todos os outros. Isto é esperado já que são utilizadas todas as variáveis e são treinadas diversas árvores para se chegar na resposta.


```{r}
library(vip)
vip(bag, num_features = 40, bar = FALSE)
```
Ainda, notamos que o preço final do índice com 1 defasagem segue com grande importância, mas já observamos os demais índices e os preços das grandes empresas americanas com mais influência na predição em comparação com os métodos anteriores. 

##### Boosting

No método de Boosting, também são treinadas diversas árvores a partir de subsets dos dados de treinamento. A diferença para o método de bagging é que estas árvores são construídas sequencialmente e o subset escolhido baseado nos resultados das predições anteriores. Assim, após escolher uma amostra do banco de dados de treinamento, treinar a árvore e fazer a pedição com os dados restantes, os valores que obtiveram resultados piores de predição ganham pesos maiores para serem escolhidos para o treinamento da árvore seguinte.

```{r, warning=F}
library(gbm)

boost = gbm(
  close ~.,
  data = train,
  distribution = "gaussian",
  n.trees = 5000,
  interaction.depth = 4
)

boost.hat = predict(boost, newdata = test)
boost.test = test$close

close.comp = data.frame(date = rep(1:20), predicted = boost.hat, actual = boost.test)

df_gathered = close.comp %>%
  gather(key = "variable", value = "value", -date)

ggplot(df_gathered, aes(x = date, y = value)) + 
  geom_line(aes(color = variable), size = 1) +
  theme_minimal() + ggtitle("Predicted vs. Actual")

paste("Média da soma dos erros quadráticos")
mean((boost.hat - boost.test)^2)
```

Observamos um erro quadrático médio menor ainda, já que o se dá um enfoque para "corrigir" os erros dos modelos anteriores.


## Questão 5: Considere o modelo $y_t = m(y_{t-1}) + \varepsilon_t$ onde $\varepsilon_t \sim IID D(0, \sigma^2)$. Escolha uma função m(.) não linear e simule uma série temporal de tamanho 500 deste modelo. Na sequência estime a função m(.) por regressão linear local e, via intervalos de confiança construídos por bootstrap para m(.), teste se um modelo linear seria adequado.

Para a estimação de uma função não paramétrica $Y_t = m(Y_{t-1}) + \varepsilon_t$, utiliza-se a regressão polinomial local, que consiste em separar a função em um grid e estimar $\beta$, por expansão de Taylor, em cada banda deste grid. A função objetivo se dá por:

$$
SQ_F = \sum_{t-1}^T\left[Y_t - \{\beta_0 + \beta_1(Y_t-y) + \dots + \beta_p(Y_t-y)^p\}\right]^2 \frac{1}{h}K\left(\frac{Y_{t-1}-y}{h}\right)
$$
onde $K$ é a função Kernel com uma distribuição simétrica centrada em zero (usualmente a normal padrão), $h$ é o que chamamos de _bandwidth_ que é qualquer número entre 0 e infinito. Com esta função, conseguimos dar peso aos pontos $Y_{t-1}$ que estão mais próximos de $y$, garantindo que estes serão utilizados para a estimação local. Observe que quanto mais próximo de zero estiver o $h$, mais próximos serão os pontos com mais peso na estimação. Por outro lado, se $h$ for muito grande, o termo $\frac{Y_{t-1}-y}{h}$ será pequeno, e pontos mais longe serão utilizados.

Simulando os dados para um modelo para $Y_t$:

```{r}
simnlts = function(n, alpha, beta, r, sigma, gamma){
  # Generate noise
  e <- rnorm(n, 0, sigma)
  # Create space for y
  y <- numeric(n)
  x = numeric(n)
  x[1] = 0
  y[1] = runif(1)
  
  # Generate y
  for(i in 2:n)
  {
    if(y[i-1] <= r)
      y[i] <- alpha*y[i-1] + e[i]
    else
      y[i] <- beta*y[i-1] + gamma*e[i]
    x[i] = y[i-1]
  }
  
  # Return result
  return(list("y" = y, "x" = x))
}
set.seed(205650)
sim = simnlts(500, 0.5, -1.8, 0.8, 1, 2)
y <- sim$y
x = sim$x
sim.data = matrix(c(y,x), byrow=F, ncol=2)
hist(y)
plot(x, y)
```



```{r, warning=F}
library(KernSmooth)
par(mfrow=c(2,2))
m1 = locpoly(x, y, degree=1, bandwidth = 1)
plot(x,y, main = "Estimação de Yt com grau=1 e h = 1")
lines(m1$x, m1$y, col = "red")
m2 = locpoly(x, y, bandwidth = 100)
plot(x,y, main = "Estimação de Yt com grau=1 e h = 100")
lines(m2$x, m2$y, col = "blue")
m3 = lm(y ~ x)
plot(x,y, main = "Estimação de Yt com regressão linear")
abline(m3, col = "orange")
m4 = locpoly(x, y, bandwidth = 0.1)
plot(x,y, main ="Estimação de Y com degree=1 e h = 0.1")
lines(m4$x, m4$y, col = "red")

```

Observa-se que, mesmo utilizando um polinômio de Taylor de grau 1, obtemos uma boa estimação da função $Y_t$, que é de ordem 4. Note que, com o $h$ muito grande, não foi possível fazer um ajuste adequado, já que praticamente não foi feita a estimação local. Ainda, com $h=100$ e com uma regressão linear obtemos o mesmo resultado, pois a função Kernel não exerce influência na escolha de pontos vizinhos de $y$. Por fim, o $h$ muito pequeno começa a demonstrar _overfitting_.


Escolhendo o $h$ ótimo:
```{r}
library(PLRModels)
hcv = np.cv(sim.data)
hcv = hcv$h.opt$matrix.0..2..num.ln.[2]
hplug = dpill(x,y)

par(mfrow=c(1,2))
m1 = locpoly(x,y, bandwidth = hcv)
plot(x,y, main = "Estimativa de Yt com h por CV")
lines(m1$x, m1$y, col = "purple")

m2 = locpoly(x,y, bandwidth = hplug)
plot(x,y, main = "Estimativa de Yt com h por plug-in")
lines(m2$x, m2$y, col = "pink")
```

```{r, warning = F}
library(simpleboot)

m = lm(y ~ x)
sb = lm.boot(m, 1000)
plot(sb)
```

Observa-se pelo intervalo de confiança gerado a partir de amostras bootstrap, o modelo linear não é adequado.



## Notes

VAR:
https://www.quantstart.com/articles/Johansen-Test-for-Cointegrating-Time-Series-Analysis-in-R/
https://bookdown.org/ccolonescu/RPoE4/vec-and-var-models.html#estimating-a-vec-model
https://corporatefinanceinstitute.com/resources/knowledge/other/cointegration/#:~:text=A%20cointegration%20test%20is%20used,time%20in%20the%20long%20term.

arrtigo lidando com clima: https://www.kaggle.com/sumanthvrao/daily-climate-time-series-data
http://www.phdeconomics.sssup.it/documents/Lesson17.pdf

ref AIC BIC: https://www.datacamp.com/community/tutorials/tutorial-ridge-lasso-elastic-net
