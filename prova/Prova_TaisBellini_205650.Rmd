---
title: "PPGEst - Prova EST0203 - Séries Temporais - Verão 2021"
author: "Tais Bellini"
date: "4/10/2021"
output: pdf_document
header-includes:
  - \DeclareMathOperator{\EX}{\mathbb{E}}
  - \DeclareMathOperator{\Z}{\mathbb{Z}}
  - \DeclareMathOperator*{\argmax}{arg\,max}


---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Questão 1: Derivar as funções de autocorrelação:

### a) De $Y_t$ que é um ARMA(1,1) estacionário.

Seja $\{ X_t \}_{t \in Z}$ um processo estocástico ARMA(1,1) estacionário, podemos representá-lo da seguinte forma:

$$
Y_t = \phi_1 Y_{t-1} + \epsilon_t + \theta_1 \epsilon_{t-1}
$$
onde $Y_t = X_t - \mu$, $\mu = \EX{X_t}$, $\{\epsilon_t\}_{t \in Z}$ ~ $RB(0, \sigma^2)$.

A função de autocorrelação ($\rho_Y(h)$) se dá pela razão entre a função de autocovariância no lag h ($\gamma_Y(h)$) e a função de autocorrelação no lag 0 ($\gamma_Y(0)$):

$$
\rho_Y(h) = \frac{\gamma_Y(h)}{\gamma_Y(0)}
$$

Portanto, para calcular a funcão de autocorrelação, precisamos da função de autocovariância:

$$ \gamma_Y(h) = Cov(Y_t, Y_{t-h}) = Cov(Y_{t+h}, Y_t)$$
Abrindo $Y_{t+h}$, obtemos:

$$
\gamma_Y(h) = Cov[(\phi_1 Y_{t+h-1} + \epsilon_{t+h} + \theta_1 \epsilon_{t+h-1}),Y_t)]
$$
$$
= \phi_1 Cov(Y_{t+h-1},Y_t) + Cov(\epsilon_{t+h}, Y_t) + \theta_1 Cov(\epsilon_{t+h-1}, Y_t)
$$

Agora note que $Cov(Y_{t+h-1}, Y_t) = \gamma_Y(h-1)$. Assim, a função de autocorrelação $\gamma_Y(h)$ se dá por: 

$$
\gamma_Y(h) = \phi_1 \gamma_Y(h-1) + Cov(\epsilon_{t+h}, Y_t) + \theta_1 Cov(\epsilon_{t+h-1}, Y_t)
$$

Observe que, para $h > 1$, teremos os termos de covarância $Cov(\epsilon_{t+h}, Y_t)$ e $\theta_1 Cov(\epsilon_{t+h-1}, Y_t)$ zerados, pois o ruído branco do futuro não interfere em $Y_t$. Assim, neste caso, as funções de autocovariância e autocorrelacão de $Y_t$ que é um ARMA(1,1) estacionário se comportam como as de um AR(1): 

$$
\gamma_Y(h) = \phi_1\gamma_Y(h-1)
$$
$$
\rho_Y(h) = \phi_1\rho(h-1)
$$
onde $h>1$.

Já para $h \leq 1$, os termos de covariância não são zerados. Para $h=0$, temos:

\begin{equation}
\gamma_Y(0) = \phi_1 \gamma_Y(-1) + Cov(\epsilon_t, Y_t) + \theta_1Cov(\epsilon_{t-1}, Y_t)
\label{gamma0_raw}
\end{equation}

Sabemos que, quando $Y_t$ é estacionário, $\gamma_Y(-h) = \gamma_Y(h)$. Ainda, 

$$
Cov(\epsilon_t, Y_t) = Cov(\epsilon_t, (\phi_1Y_{t-1} + \epsilon_t + \theta_1\epsilon_{t-1}))
$$
$$
= \phi_1Cov(\epsilon_t, Y_{t-1}) + Cov(\epsilon_t, \epsilon_t) + \theta_1Cov(\epsilon_t, \epsilon_{t-1})
$$
onde $Cov(\epsilon_t,Y_{t-1}) = 0$, pois $Y_{t-1}$ não depende de $\epsilon$ no tempo presente. Como $\epsilon_t$ é ruído branco, $Cov(\epsilon_t, \epsilon_{t-1})=0$ e $Cov(\epsilon_t, \epsilon_t) = \sigma^2$ logo, podemos expressar

\begin{equation}
Cov(\epsilon_t, Y_t)  = \sigma^2
\label{cov_et_yt}
\end{equation}

Para determinarmos a covariância entre $\epsilon_{t-1}$ e $Y_t$, temos que:
\begin{equation}
Cov(\epsilon_{t-1}, Y_t) = \phi_1Cov(\epsilon_{t-1}, Y_{t-1}) + Cov(\epsilon_{t-1}, \epsilon_t) + \theta_1 Cov(\epsilon_{t-1}, \epsilon_{t-1})
\label{cov_et-1_yt_raw}
\end{equation}
onde $Cov(\epsilon_{t-1}, \epsilon_t) = 0$, $Cov(\epsilon_{t-1}, \epsilon_{t-1}) = \sigma^2$ e 
$$
Cov(\epsilon_{t-1}, Y_{t-1}) = Cov(\epsilon_{t-1}, (\phi_1Y_{t-2} + \epsilon_{t-1} + \theta_1\epsilon_{t-2})) = \phi_1Cov(\epsilon_{t-1}, Y_{t-2}) + Cov(\epsilon_{t-1},\epsilon_{t-1}) + \theta_1Cov(\epsilon_{t-1}, \epsilon_{t-2})
$$
no qual, pelas propriedades do processo e do ruído branco, $Cov(\epsilon_{t-1}, Y_{t-2}) = 0$, $Cov(\epsilon_{t-1}, \epsilon_{t-2}) = 0$ e $Cov(\epsilon_{t-1},\epsilon_{t-1}) = \sigma^2$. Portanto, 
\begin{equation}
Cov(\epsilon_{t-1}, Y_{t-1}) = \sigma^2
\label{cov_etmin1_ytmin1}
\end{equation}

Substituindo \ref{cov_etmin1_ytmin1} em \ref{cov_et-1_yt_raw} e considerando as propriedades do processo e de ruído branco, temos 
\begin{equation}
Cov(\epsilon_{t-1}, Y_t) = \phi_1\sigma^2 + \theta_1\sigma^2
\label{cov_et-1_yt}
\end{equation}

Finalmente, substituindo \ref{cov_et-1_yt} e \ref{cov_et_yt} em \ref{gamma0_raw}, temos:
$$
\gamma_Y(0) = \phi_1\gamma_Y(1) + \sigma^2 + \theta_1 (\phi_1 + \theta_1)\sigma^2
$$
Para $h=1$, determinamos que:

\begin{equation}
\gamma_Y(1) = \phi_1 \gamma_Y(0) + Cov(\epsilon_{t+1}, Y_t) + \theta_1 Cov(\epsilon_{t}, Y_t) = \phi_1 \gamma_Y(0) + \theta_1\sigma^2
\label{gamma1}
\end{equation}

Resolvendo $\gamma_Y(0)$ e $\gamma_Y(1)$ simultaneamente, concluímos que:

$$
\gamma_Y(0) = \frac{2\phi_1\theta_1 + 1 + \theta_1^2}{1-\phi_1^2}\sigma^2
$$
e
$$
\gamma_Y(1) =  \frac{(\phi_1^2\theta_1 + \theta_1 + \phi_1\theta_1^2 + \theta_1)\sigma^2}{1-\phi_1^2} = \frac{(1+\phi_1\theta_1)(\phi_1+\theta_1)}{1-\phi_1^2}\sigma^2
$$
Portanto, para $h \leq 1$,
$$
\rho_Y(0) = 1
$$

$$
\rho_Y(1) = \frac{\gamma_Y(1)}{\gamma_Y(0)} = \frac{(1+\phi_1\theta_1)(\phi_1+\theta_1)}{2\phi_1\theta_1 + 1 + \theta_1^2} 
$$
e para $h > 1$:
$$
\rho_Y(h) = \phi_1\rho(h-1)
$$

### b) De $Y_t^2$, onde $Y_t$ é um ARCH(2) estacionário.

Dado que $Y_t$ é um ARCH(2) estacionário, temos o seguinte modelo:

\begin{equation}
Y_t = \sigma_t\epsilon_t
\label{yt}
\end{equation}

onde $\epsilon_t \sim RB(0,1)$ e $\sigma_t^2$ é a variância condicional de $Y_t$ dadas as informações que temos até o instante $\Im_{t-1}$ que tem a seguinte estrutura:
\begin{equation}
\sigma^2 = \alpha_0 + \alpha_1Y_{t-1}^2 + \alpha_2Y_{t-2}^2
\label{sigma2}
\end{equation}

Para determinar a função de autocorrelação do $Y_t^2$, vamos elevar \ref{yt} ao quadrado em ambos os lados:

$$
Y_t^2 = \sigma_t^2\epsilon_t^2
$$

Somando e subtraindo $\sigma^2$ da equação, obtemos a seguinte estrutura:

$$
Y_t^2 = \alpha_0 + \alpha_1Y_{t-1}^2 + \alpha_2Y_{t-2}^2 + \sigma^2(\epsilon_t^2-1)
$$

Observe que $\EX{[\sigma_t^2(\epsilon_t^2-1)]} = 0$: 
$$
\EX{[\sigma_t^2(\epsilon_t^2-1)]} = \EX{\sigma_t^2}\EX{[\epsilon_t^2 - 1]}
$$
onde
$$
\EX{[\epsilon_t^2 - 1]} = \EX{\epsilon_t^2} - 1 = Var(\epsilon_t) + [\EX{\epsilon}]^2 - 1 = 1 + 0 -1 = 0
$$

Ainda, $Var(\sigma_t^2(\epsilon_t^2-1)) = Var(\epsilon_t^2-1)\EX{\sigma^2}^2$ e, sob a condição de que \ref{yt} e \ref{sigma2} têm quarto momento definido,  $Cov(\sigma_t^2(\epsilon_t^2-1), \sigma_{t-k}^2(\epsilon_{t-k}^2-1)) = 0$ para $k = 1, 2, \dots$. Assim, concluímos que $\sigma_t^2(\epsilon_t^2-1)$ é um ruído branco, e a estrutura de $Y_t^2$ se dá por: 

\begin{equation}
Y_t^2 = \alpha_0 + \alpha_1Y_{t-1}^2 + \alpha_2Y_{t-2}^2 + \varepsilon_t
\label{yt2}
\end{equation}
onde $EX{\varepsilon_t} = 0$ e $Var(\varepsilon_t) = Var(\epsilon_t^2-1)\EX{\sigma^2}^2 = \Sigma$.

Portanto, temos que $Y_t^2$ é um processo AR(2). Assim, sua função de autocorrelação ($\rho_{Y^2}$) é derivada da autocovariância ($\gamma_{Y^2}$):

$$
\gamma_{Y^2}(k) = \EX{[(Y_t^2- \mu), (Y_{t-k}^2 - \mu)]} = \EX{(Y_t^2Y_{t-k}^2)}
$$
$$
= \EX{[(\alpha_1Y_{t-1}^2 + \alpha_2Y_{t-2}^2 + \varepsilon_t)Y_{t-k}^2]}
$$
$$
= \alpha_1\EX{[Y_{t-1}^2Y_{t-k}^2]} + \alpha_2\EX{[Y_{t-2}^2Y_{t-k}^2]} + \EX{[\varepsilon_tY_{t-k}^2]}
$$
Ou seja, para $k=0$:
$$
\gamma_{Y^2}(k) = \alpha_1\gamma_{Y^2}(k-1) + \alpha_2\gamma_{Y^2}(k-2) + \Sigma
$$
E para $k>0$:
$$
\gamma_{Y^2}(k) = \alpha_1\gamma_{Y^2}(k-1) + \alpha_2\gamma_{Y^2}(k-2)
$$
Portanto, as condições iniciais se dão pela resolução das seguintes equações simultaneamente:

$$
\gamma_{Y^2}(0) = \alpha_1\gamma_{Y^2}(1) + \alpha_2\gamma_{Y^2}(2) + \Sigma
$$
$$
\gamma_{Y^2}(1) = \alpha_1\gamma_{Y^2}(0) + \alpha_2\gamma_{Y^2}(1)
$$
$$
\gamma_{Y^2}(2) = \alpha_1\gamma_{Y^2}(1) + \alpha_2\gamma_{Y^2}(0)
$$
Logo, temos:

$$
\gamma_{Y^2}(0) = \left(\frac{1-\alpha_2}{1+\alpha_2}\right)\frac{\Sigma}{(1-\alpha_2)^2-\alpha_1^2}
$$
$$
\gamma_{Y^2}(1) = \frac{\alpha_1}{1-\alpha_2}\gamma_{Y_t^2}(0) = \left(\frac{\alpha_1}{1-\alpha_2}\right)\left(\frac{1-\alpha_2}{1+\alpha_2}\right)\frac{\Sigma}{(1-\alpha_2)^2-\alpha_1^2}
$$
A partir disto, podemos derivar $\gamma_{Y^2}(k)$ para $k = 2, 3, \dots$:
$$
\gamma_{Y^2}(k) = \alpha_1\gamma_{Y^2}(k-1) + \alpha_2\gamma_{Y^2}(k-2)
$$
Finalmente, derivamos as funções de autocorrelação ($\rho_{Y_t^2}(k)$): 

$$
\rho_{Y_t^2}(1) = \frac{\gamma_{Y^2}(1)}{\gamma_{Y^2}(0)} = \frac{\alpha_1}{1-\alpha_2}
$$
$$
\rho_{Y_t^2}(2) = \alpha_1\rho_{Y_t^2}(1) + \alpha_2 = \frac{\alpha_1^2+(1-\alpha_2)\alpha_2}{1-\alpha_2}
$$
Para $k = 3, 4, \dots$:
$$
\rho_{Y_t^2}(k) = \alpha_1\rho_{Y_t^2}(k-1) + \alpha_2\rho_{Y_t^2}(k-2)
$$

## Questão 2

### a) Encontre a distribuição assintótica do estimador de máxima versossimilhança (EMV) de $\phi_1$ no modelo $y_t = \phi_0 + \phi_1y_{t-1} + \epsilon_t$ onde $\epsilon_t \sim N(0, \sigma^2)$ Use o resultado geral para os EMV's e explicite qualquer suposição adicional necessária.

Seja $\theta = (\phi_1, \dots, \phi_p, \sigma^2)$ o vetor dos parâmetros populacionais que deseja-se estimar e $\mathbf{Y} = (Y_1, \dots, Y_{T-1}, Y_T)$ um amostra observada. A função densidade de probabilidade (FDP) conjunta é denotada por:
\begin{equation}
f(Y_T, Y_{T-1} \dots, Y_1;\theta)
\label{fdp}
\end{equation}

Logo, a função de máxima verossimilhança é dada por:
\begin{equation}
L(\theta|\mathbf{Y}) = f(Y_T, Y_{T-1} \dots, Y_1;\theta)
\label{fmv}
\end{equation}

E o estimador de log-máxima verossimilhança (EMV) será o argumento que maximiza a função de máxima verossimilhança:
$$
\hat{\theta}_{EMV} = \arg \max_{\theta} l(\theta|\mathbf{Y})
$$
onde $l(\theta|\mathbf{y}) = ln(L(\theta|\mathbf{y}))$.

Assumindo que a derivada de $l(\theta|\mathbf{y})$ em relação a $\theta$ existe e é contínua, determinamos o $\hat{\theta}_{EMV}$ resolvendo:
$$
\frac{\delta l(\theta|\mathbf{Y})}{\delta \theta} = 0
$$
Agora, considerando o modelo AR(p):
\begin{equation}
Y_t = \phi_0 + \phi_1Y_{t-1} + \dots + \phi_pY_{t-p} + \epsilon_t
\label{arp}
\end{equation}
onde $\epsilon_t \sim i.i.d\ N(0, \sigma^2)$, e $Y_{t-1} \dots, Y_{t-p}$ valores do passado já observados, podemos considerar que $\phi_0 + \phi_1Y_{t-1} + \dots + \phi_pY_{t-p}$ é um valor constante que está sendo somado a $\epsilon_t$ e, portanto:
$$
Y_t|\Im_{t-1} \sim N(\phi_0 + \phi_1Y_{t-1} + \dots + \phi_pY_{t-p}, \sigma^2)
$$
Ainda, sabendo que uma FDP conjunta qualquer $f(X,Y)$ pode ser decomposta em $f(Y|X)f(X)$ e conhecendo a distribuicão de $Y_t|\Im_{t-1}$ podemos reescrever \ref{fmv} como:
$$
L(\theta|\mathbf{Y}) = f(Y_T|Y_1, \dots, Y_{T-1};\theta)f(Y_{T-1} \dots, Y_1;\theta)
$$
E assim podemos recursivamente decompor a função conjunta do segundo termo de forma a obter o produto das FDP condicionais de T-1 até 2, e ter FDP marginal de $Y_1$ no segundo termo.

Agora 

Ainda, no caso de $Y_1$, vamos retomar o fato de que, para um processo estacionário AR(1):
$$
\EX{Y_1} = \mu = \frac{\phi_0}{1-\phi_1}
$$
$$
Var(Y_1) = \frac{\sigma^2}{1-\phi_1^2}
$$
Portanto, temos que $Y_1 \sim N(\frac{\phi_0}{1-\phi_1}, \frac{\sigma^2}{1-\phi_1^2})$.

Olhando especificamente para o modelo AR(1):
\begin{equation}
Y_t = \phi_0 + \phi_1Y_{t-1} + \epsilon_t
\label{ar1}
\end{equation}

A decomposição da função de máxima verossimilhança é feita da seguinte forma:

$$
L(\theta|\mathbf{Y}) = f(Y_t|Y_{t-1};\theta)f(Y_{t-1};\theta)
$$
https://faculty.washington.edu/ezivot/econ584/notes/armaestimation.pdf
http://www.science.unitn.it/AnalisiInfoTLC/SSP/SSP14_15/lectures/lecture15.pdf
https://math.unice.fr/~frapetti/CorsoP/Chapitre_4_IMEA_1.pdf
http://www-stat.wharton.upenn.edu/~stine/stat910/lectures/12_est_arma.pdf
http://course.sdu.edu.cn/G2S/eWebEditor/uploadfile/20140110134920017.pdf

## Questão 3

### a) Mostre que estimar um VAR(p) padrão de dimensão n (não estrutural) por MV é equivalente a estimar por MQO linha por linha (das n linhas) do modelo.

Dado um modelo multivariado:

\begin{equation}
\mathbf{y_t} = \mathbf{c} + \Phi_1\mathbf{y_{t-1}} + \dots + \Phi_p\mathbf{y_{t-p}} + \mathbf{\epsilon_t}
\label{eq:var}
\end{equation}

onde $\mathbf{y_t} = \begin{bmatrix} y_{1t} \\ \vdots \\ y_{nt} \end{bmatrix}$ é um vetor contendo $\mathbf{y_t}$ para cada uma das n variáveis, $\mathbf{c} = \begin{bmatrix} c_1 \\ \vdots \\ c_n \end{bmatrix}$ é um vetor de constantes, $\Phi_j = \begin{bmatrix} \phi_{11}^{(j)} & \dots & \phi_{1n}^{(j)} \\ \vdots \\ \phi_{n1}^{(j)} & \dots & \phi_{nn}^{(j)} \end{bmatrix}$ é uma matriz com os parâmetros para todas as variáveis com aquela defasagem e $\mathbf{\epsilon_t} \sim N(\mathbf{0}, \Omega)$ é um vetor de ruídos. 

Dessa forma, temos um sistema de equações no qual a j-ésima linha se dá por: 

\begin{equation}
\begin{aligned}
y_{jt} = c_j & + \phi_{j1}^{(1)}y_{1,t-1} + \dots + \phi_{jn}^{(1)}y_{n,t-1} +
             & + \phi_{j1}^{(2)}y_{1,t-2} + \dots + \phi_{jn}^{(2)}y_{n,t-2} + \dots 
             & + \phi_{j1}^{(p)}y_{1,t-p} + \dots + \phi_{jn}^{(p)}y_{n,t-p} + \epsilon_{jt} 
\label{eq:varj}
\end{aligned}
\end{equation}

Dado o sistema da equação \ref{eq:var}, consideramos o vetor $\mathbf{c_t} = \mathbf{c} + \Phi_1\mathbf{y_{t-1}} + \dots + \Phi_p\mathbf{y_{t-p}}$, onde $y_t|\Im_{t-1} \sim NM(\mathbf{c_t}, \Omega)$, e o vetor $\mathbf{\theta}  = (\mathbf{c}, \Phi_1, \dots, \Phi_p, \Omega)$. 
O estimador de máxima verossimilhança desde sistema, como visto em aula, é definido da seguinte forma:

\begin{equation}
L(\mathbf{\theta}) = \prod_{t=1}^{T}f(\mathbf{y_t}|\Im_{t-1};\theta) = (2\pi)^{-Tn/2}|\Omega^{-1}|exp\{-\frac{1}{2}\sum_{t-1}^T(\mathbf{y_t}-\mathbf{c_t})^\intercal\Omenga^{-1}(\mathbf{y_t}-\mathbf{c_t})}
\label{eq:lvar}
\end{equation}

Denotamos, ainda: 
$$
\mathbf{c_t} = \Pi^\intercal\mathbf{x_t}
$$
onde $\Pi^\intercal = (\mathbf{c}, \Phi_1, \dots, \Phi_p)$ e 
$$
\mathbf{x_t} = \begin{bmatrix} 1 \\ \mathbf{y_{t-1}} \\ \vdots \\ \mathbf{y_{t-p}} \end{bmatrix}
$$
Fazendo a derivação em relação a $\Pi$ do logaritmo natural de \ref{eq:lvar} para obter o estimador de MV dos parâmetros do modelo, obtemos o seguinte resultado:
$$
\hat{\Pi}^\intercal = \frac{\left[\sum_{t=1}^T\mathbf{y_t}\mathbf{x_t}^\intercal\right]}{\left[\sum_{t=1}^T\mathbf{x_t}\mathbf{x_t}^\intercal\right]}
$$
Note que a j-ésima linha de $\hat{\Pi}^\intercal$ é definida por: 

\begin{equation}
\hat{\pi_j}^\intercal = \frac{\left[\sum_{t=1}^Ty_{jt}\mathbf{x_t}^\intercal\right]}{\left[\sum_{t=1}^T\mathbf{x_t}\mathbf{x_t}^\intercal\right]}
\label{eq:varmv}
\end{equation}

Ou, em sua forma vetorial: 

\begin{equation}
\hat{\pi_j}^\intercal = \frac{X^\intercal y_j}{(X^\intercal X)}
\label{eq:varmvvet}
\end{equation}

onde 
$$
y_{j} = (y_{j1}, \dots, y_{jT})^\intercal
$$
$$
X = \begin{bmatrix} 
      1 & y_{1,0} & \dots & y_{n,0} \\
      \vdots & & & \vdots \\
      1 & y_{1,t-1} & \dots & y_{n,t-1}
      \end{bmatrix}
$$

Agora, observe a equação da j-ésima linha em \ref{eq:varj}. Sua forma vetorial se dá por: 

$$
y_{j} = X\beta_j + \epsilon_j
$$
onde 
$$
y_{j} = (y_{j1}, \dots, y_{jT})^\intercal
$$
$$
X = \begin{bmatrix} 
      1 & y_{1,0} & \dots & y_{n,0} \\
      \vdots & & & \vdots \\
      1 & y_{1,t-1} & \dots & y_{n,t-1}
      \end{bmatrix}
$$
$$
\beta_j = (c_1, \phi_{j1}, \dots, \phi_{jn})^\intercal
$$
e
$$
\epsilon_j = (\epsilon_{j1}, \dots, \epsilon_{jT})^\intercal
$$

Se obtivermos o estimador de mínimos quadrados (OLS) para estimar $\beta_j$ da equação acima, temos:
$$
\hat{\beta_j} = \frac{X^\intercal y_j}{(X^\intercal X)}
$$
Que é exatamente equivalente a equação \ref{eq:varmvvet}.

Para a estimação de $\Omega$, temos:

$$
\hat{\Omega} = \frac{1}{T} \sum_{t=1}^{T}\mathbf{\hat{\epsilon_t}^\intercal\hat{\epsilon_t}}
$$
onde
$$
\mathbf{\hat{\epsilon_t}} = \begin{bmatrix} \hat{\epsilon_{1t}} \\ \vdots \\ \hat{\epsilon_{nt}} \end{bmatrix}
$$
e $\hat{\epsilon_{jt}}$ são os resíduos do ajuste de mínimos quadrados da j-ésima equação.

### b) Com uma série temporal vetorial multivariada a sua escolha (mas com dimensão mínima de 3), estime um VAR ou VEC (o que for mais adequado – faça testes),mostrando os resultados e os interpretando.


```{r}
#library(tsDyn)
#library(vars)
#data = read.csv2("SingaporeData.csv", sep = ",")

#GDP = ts(data$lnGDP, start = c(2003,1,31), frequency = 4)
#CPI = ts(data$lnCPI, start = c(2003,1,31), frequency = 4)
#M4 = ts(data$lnM4, start = c(2003,1,31), frequency = 4)

#ds <- cbind(GDP,CPI,M4)

#Selecting the Optimal Number of Lags (Recall, this is p - 1)

#lagselect = VARselect(ds, lag.max = 6, type = "const")
#lagselect$selection
#lagselect$criteria

#ctest1t <- ca.jo(ds, type = "trace", ecdet = "const", K = 4)
#summary(ctest1t)

#ctest1e <- ca.jo(ds, type = "eigen", ecdet = "const", K = 4)
#summary(ctest1e)

#Hence, we have one cointegrating relationship in this model

######################################################################

#Build the VECM Model

#Model1 <- VECM(ds, 4, r = 1, estim =("2OLS"))
#summary(Model1)

#Diagnostic Tests

#Need to Transform VECM to VAR

#Model1VAR <- vec2var(ctest1t, r = 1)

#Serial Correlation

#Serial1 <- serial.test(Model1VAR, lags.pt = 5, type = "PT.asymptotic")
#Serial1

#ARCH Effects

#Arch1 <- arch.test(Model1VAR, lags.multi = 15, multivariate.only = TRUE)
#Arch1

#Normality of Residuals

#Norm1 <- normality.test(Model1VAR, multivariate.only = TRUE)
#Norm1

#Impulse Response Functions

#M3irf <- irf(Model1VAR, impulse = "GDP", response = "M4", n.ahead = 20, boot = TRUE)
#plot(M3irf, ylab = "M3", main = "GDP's shock to M3")

#CPIirf <- irf(Model1VAR, impulse = "GDP", response = "CPI", n.ahead = 20, boot = TRUE)
#plot(CPIirf, ylab = "CPI", main = "GDP's shock to CPI")

#GDPirf <- irf(Model1VAR, impulse = "GDP", response = "GDP", n.ahead = 20, boot = TRUE)
#plot(GDPirf, ylab = "GDP", main = "GDP's shock to GDP")

#Variance Decomposition

#FEVD1 <- fevd(Model1VAR, n.ahead = 10)
#plot(FEVD1)

```

Para esta questão, será usado um banco de dados de 1462 observações que mede o clima diário da cidade de Delhi, na Índia, no período de primeiro de Janeiro de 2013 a 24 de Abril de 2017 medindo os seguintes parâmetros: temperatura média, umidade, velocidade do vento e pressão média do ar. 

```{r, echo = F, include = F}
library(tseries) # for ADF unit root tests
library(urca)
library(vars)
library(mFilter)
library(forecast)
```

Análise exploratória dos dados:

```{r}
data = read.csv("DailyDelhiClimateTrain.csv", sep=",")
data = data[data$meanpressure > 990 & data$meanpressure < 1300,] #remove outliers
data = data[data$wind_speed < 30,] #remove outliers
ts_data = ts(data)
temp = ts(ts_data[,2])
humidity = ts(ts_data[,3])
wind = ts(ts_data[,4])
pressure = ts(ts_data[,5])
plot(scale(ts_data[,-1]), main = "Plot de cada série do banco de dados",  yax.flip = TRUE)
```

Para uma análise de séries temporais multivariadas, temos o modelo VAR, cuja fórmula geral VAR(p) tem a seguinte estrutura:

$$
Y_t = \mathbf{\mu} + A_1\mathbf{Y_{t-1}} + \dots + A_p\mathbf{Y_{t-p}} + \mathbf{\epsilon_t}
$$
onde $\mu$ é o vetor de médias da série, $A_i$ são as matrizes dos coeficientes para cada lag e $\epsilon_t$  é vetor de erros com distribuição normal multivariada com média zero. 

Para utilizá-lo, é necessário avaliar se as séries a serem trabalhadas são estacionárias:

```{r, warning=F}
par(mfrow=c(2,2))
acf(temp, main = "ACF Temperatura Média")
acf(humidity, main = "ACF Umidade")
acf(wind, main = "ACF Velocidade do vento")
acf(pressure, main = "ACF Pressão do ar Média")

par(mfrow=c(2,2))
pacf(temp)
pacf(humidity)
pacf(wind)
pacf(pressure)

```

Nota-se que as variáveis média de temperatura, umidade e pressão do ar parecem ter um componente de sazonalidade.

```{r, warning = F}
t = adf.test(temp)
h = adf.test(humidity)
w = adf.test(wind)
p = adf.test(pressure)

adf_pvalues = c(t$p.value, h$p.value, w$p.value, p$p.value)
names(adf_pvalues) = c("Temp", "Umidade", "Vento", "Pressao")
paste("P-Value do teste ADF")
adf_pvalues
```

Observamos que apenas a série da valocidade do vendo possui p-valor < 0.01. Assim, utilizaremos as demais variáveis com uma diferença para modelar o VAR:

```{r, warning = F}
temp.diff= diff(temp, differences = 1)
plot(temp.diff)

humidity.diff = diff(humidity, differences = 1)
plot(humidity.diff)

pressure.diff = diff(pressure, differences = 1)
plot(pressure.diff)

t = adf.test(temp.diff)
h = adf.test(humidity.diff)
w = adf.test(wind)
p = adf.test(pressure.diff)

adf_pvalues = c(t$p.value, h$p.value, w$p.value, p$p.value)
names(adf_pvalues) = c("Temp", "Umidade", "Vento", "Pressao")
paste("P-Value do teste ADF")
adf_pvalues
```

Vamos remover as últimas 7 observações para avaliar o ajuste do modelo depois:

```{r}
ts_data_diff = ts(cbind(temp.diff, humidity.diff, pressure.diff))
test = ts_data_diff[1:7,]
train = ts_data_diff[8:nrow(ts_data_diff),]

VARselect(train, lag.max = 20)

ca = ca.jo(ts_data_diff)
summary(ca)
```

```{r}
#model = VAR(train, p=6, type = "trend")
#summary(model)
#summary(model$varresult$wind)$adj.r.squared

#serial.test(model, lags.pt = 15, type = "PT.asymptotic")

```


Observa-se que apenas 2 variáveis são séries estacionárias. Portanto, vamos testar a co-integração através do método dos autovalores de Johansen. 

Para isto, temos o modelo VEC, que faz a diferenciação das séries obtendo a seguinte forma:

$$
\Delta \mathbf{Y_t} = \mathbf{\mu} + A\mathbf{Y_{t-1}} + \Gamma_1\Delta \mathbf{Y_{t-1}} + \dots + \Gamma_p\Delta \mathbf{Y_{t-p}} + \mathbf{\epsilon_t}
$$
onde $\Delta \mathbf{Y_{t}} = \mathbf{Y_t} - \mathbf{Y_{t-1}}$ é o operador de diferenciação, A é a matriz de coeficientes para o primeiro lag e $\Gamma_i$ as matrizes dos coeficientes para cada lag. 

O teste de Johansen verifica sequencialmente se o posto da matriz é 0, 1, 2, ... até o número de séries sendo avaliadas menos um. Se o posto da matriz A = 0, não há co-integração.

```{r}
jotest = ca.jo(ts_data[,-1], spec = "longrun")
summary(jotest)

lambda = c(0.245989042, 0.063838185, 0.009330528)/0.331000093

temperature = lambda[1]*humidity + lambda[2]*wind + lambda[3]*pressure
plot(temperature, type = "l")
adf.test(temperature)
```


## Questão 4: Utilizando uma série temporal a sua escolha (mas com pelo menos 10 séries temporais como covariáveis e pelo menos 3 lags de cada uma delas, ou seja, pelo menos 30 covariáveis), implemente, para estimação e seleção, todos métodos vistos em aula sobre machine/statistical learning: ridge, lasso, adalasso, tress (baging, random forest e boosting). Mostre os resultados e os interprete.

Para esta questão, será utilizado um banco de dados utilizado no paper https://data.mendeley.com/datasets/byjnr4kz5v/3 que contém recursos de várias categorias de indicadores técnicos, contratos futuros, preços de commodities, índices importantes de mercados em todo o mundo, preços das principais empresas no mercado dos EUA e taxas de títulos do tesouro. Foram selecionados 10 ativos financeiros: Oil, Gold, JPM, AMZN, AAPL, GE, DJI, NYSE, EUR, DOLLAR e o preço de fechamento (Close) do índice S&P.

```{r, warning=F, echo=F, include=F}
library(ggplot2)
library(tidyr)

```


```{r}
data = read.csv2("Processed_S&P.csv", sep = ",")

oil = ts(as.numeric(data$Oil))
gold = ts(as.numeric(data$Gold))
jpm = ts(as.numeric(data$JPM))
close = ts(as.numeric(data$Close))
amzn = ts(as.numeric(data$AMZN))
ge = ts(data$GE)
dji = ts(data$DJI)
nyse = ts(data$NYSE)
eur = ts(data$EUR)
aapl = ts(data$AAPL)
dollar = ts(data$Dollar.index)

df = data.frame(close, oil, gold, jpm, amzn, aapl, ge, dji, nyse, eur, dollar)
lines = nrow(df)
df = within(df, {
  close1 = c(rep(NA,1), close[-seq(from = lines, by = -1, length.out = 1)])
  close2 = c(rep(NA,2), close[-seq(from = lines, by = -1, length.out = 2)])
  close3 = c(rep(NA,3), close[-seq(from = lines, by = -1, length.out = 3)])
  oil1 = c(rep(NA,1), oil[-seq(from = lines, by = -1, length.out = 1)])
  oil2 = c(rep(NA,2), oil[-seq(from = lines, by = -1, length.out = 2)])
  oil3 = c(rep(NA,3), oil[-seq(from = lines, by = -1, length.out = 3)])
  gold1 = c(rep(NA,1), gold[-seq(from = lines, by = -1, length.out = 1)])
  gold2 = c(rep(NA,2), gold[-seq(from = lines, by = -1, length.out = 2)])
  gold3 = c(rep(NA,3), gold[-seq(from = lines, by = -1, length.out = 3)])
  jpm1 = c(rep(NA,1), jpm[-seq(from = lines, by = -1, length.out = 1)])
  jpm2 = c(rep(NA,2), jpm[-seq(from = lines, by = -1, length.out = 2)])
  jpm3 = c(rep(NA,3), jpm[-seq(from = lines, by = -1, length.out = 3)])
  amzn1 = c(rep(NA,1), amzn[-seq(from = lines, by = -1, length.out = 1)])
  amzn2 = c(rep(NA,2), amzn[-seq(from = lines, by = -1, length.out = 2)])
  amzn3 = c(rep(NA,3), amzn[-seq(from = lines, by = -1, length.out = 3)])
  aapl1 = c(rep(NA,1), aapl[-seq(from = lines, by = -1, length.out = 1)])
  aapl2 = c(rep(NA,2), aapl[-seq(from = lines, by = -1, length.out = 2)])
  aapl3 = c(rep(NA,3), aapl[-seq(from = lines, by = -1, length.out = 3)])
  ge1 = c(rep(NA,1), ge[-seq(from = lines, by = -1, length.out = 1)])
  ge2 = c(rep(NA,2), ge[-seq(from = lines, by = -1, length.out = 2)])
  ge3 = c(rep(NA,3), ge[-seq(from = lines, by = -1, length.out = 3)])
  dji1 = c(rep(NA,1), dji[-seq(from = lines, by = -1, length.out = 1)])
  dji2 = c(rep(NA,2), dji[-seq(from = lines, by = -1, length.out = 2)])
  dji3 = c(rep(NA,3), dji[-seq(from = lines, by = -1, length.out = 3)])
  nyse1 = c(rep(NA,1), nyse[-seq(from = lines, by = -1, length.out = 1)])
  nyse2 = c(rep(NA,2), nyse[-seq(from = lines, by = -1, length.out = 2)])
  nyse3 = c(rep(NA,3), nyse[-seq(from = lines, by = -1, length.out = 3)])
  dollar1 = c(rep(NA,1), dollar[-seq(from = lines, by = -1, length.out = 1)])
  dollar2 = c(rep(NA,2), dollar[-seq(from = lines, by = -1, length.out = 2)])
  dollar3 = c(rep(NA,3), dollar[-seq(from = lines, by = -1, length.out = 3)])
  eur1 = c(rep(NA,1), eur[-seq(from = lines, by = -1, length.out = 1)])
  eur2 = c(rep(NA,2), eur[-seq(from = lines, by = -1, length.out = 2)])
  eur3 = c(rep(NA,3), eur[-seq(from = lines, by = -1, length.out = 3)])
})

df = df[-c(1:4),]
head(df)
```

#### Ridge

A regressão Ridge é uma regressão de mínimos quadrados ordinários, mas que realiza o _shrinkage_ dos coeficientes a partir de uma penalização na soma dos seus quadrados, impondo que esta soma seja menos que um valor definido $t$. Assim, temos que: 

$$
\hat{\beta}_{ridge} = \arg \min_{\beta_0, \dots, \beta_k}  \Bigg\{\sum_{i=1}^n\left(y_i - \beta_0 - \sum_{j=1}^k\beta_j x_{ij}\right) + \lambda\sum_{j=1}^k\beta_j^2\Bigg\}
$$

O resultado são coeficientes com valores menores do que uma regressão de mínimos quadrados tradicional (OLS).

Vamos determinar a variável resposta e centralizá-la, já que o algoritmo de regressão Ridge parte do pressuposto que os preditores estão padronizados e resposta centralizada. As covariáveis do banco já estão na mesma unidade, não sendo necessária a padronização.

```{r}
# variavel resposta:
y = df$close
y = scale(y, center = T)
#covariaveis
X = as.matrix(df[,-1])
X = scale(X)
```


Vamos observar como fica o ajuste em uma regressão de mínimos quadrados tradicional:
```{r}
library(knitr)
#OLS
fit.OLS = lm(formula = y~X)
B.hat.OLS = fit.OLS$coefficients
kable(B.hat.OLS)
```


Executando a regressão ridge com lambda fixo, e os lambdas escolhidos por AIC, BIC e cross-validation:

``` {r}
library(glmnet)
lambdas_to_try <- 10^seq(-2, 4, length.out = 100)
lambda_cv = cv.glmnet(X, y, alpha = 0, lambda = lambdas_to_try, standardize = T, nfolds = 10)$lambda.min
lambda_fixed = 0.1

#Ridge Regression - alpha = 0 
fit.RR.CV = glmnet(y=y,x=X, lambda=lambda_cv, alpha = 0)
fit.RR.Fixed = glmnet(y=y,x=X, lambda=lambda_fixed, alpha = 0)

#Coeficientes do RR
B.hat.RR.CV = as.vector(fit.RR.CV$beta)
B.hat.RR.Fixed = as.vector(fit.RR.Fixed$beta)

#Intercepto do RR
B0.hat.RR.CV = fit.RR.CV$a0
B0.hat.RR.Fixed = fit.RR.Fixed$a0

B.hat.RR.CV = c(B0.hat.RR.CV, B.hat.RR.CV)
B.hat.RR.Fixed = c(B0.hat.RR.Fixed, B.hat.RR.Fixed)
names(B.hat.RR.CV) = names(B.hat.RR.Fixed) = names(B.hat.OLS)

lambdas = cbind(0, lambda_cv, lambda_fixed)
coefs = cbind(B.hat.OLS, B.hat.RR.CV, B.hat.RR.Fixed)
rownames(lambdas) = "Lambda value"
kable(rbind(coefs, lambdas))

```

Conseguimos observar que o $\lambda$ escolhidos por cross-validation, 0.01, reduziu os coeficientes  e o fixado no valor 5 reduziram moderadamente os coeficientes. Estes resultados eram esperados, já que o valor dos coeficientes diminui a medida que o $\lambda$ aumenta, como observado no gráfico abaixo.


```{r}
res <- glmnet(X, y, alpha = 0, lambda = lambdas_to_try, standardize = T)
plot(res, xvar = "lambda")
```
Agora, vamos avaliar o ajuste do modelo obtido de acordo com cada lambda escolhido:

```{r}
library(tidyverse)
y_hat_ols = predict(fit.OLS)
ssr_ols = t(y - y_hat_ols) %*% (y - y_hat_ols)

y_hat_cv = predict(fit.RR.CV, X)
ssr_cv = t(y - y_hat_cv) %*% (y - y_hat_cv)

y_hat_fixed = predict(fit.RR.Fixed, X)
ssr_fixed = t(y - y_hat_fixed) %*% (y - y_hat_fixed)


y_hat = ts(cbind(y,  y_hat_ols, y_hat_cv, y_hat_fixed))
colnames(y_hat) = c("real", "ols", "cv", "fixed")
df_y <- data.frame(y_hat)
df_y$date <- as.numeric(row.names(df_y))


df_gathered = df_y %>%
  gather(key = "variable", value = "value", -date)

ggplot(df_gathered, aes(x = date, y = value)) + 
  geom_line(aes(color = variable), size = 1) +
  theme_minimal()

ssr = cbind(ssr_ols, ssr_cv, ssr_fixed)
colnames(ssr) = c("ols", "cv", "fixed")
rownames(ssr) = "SSR"
kable(ssr)
```

```{r}
df_gathered = df_y[,c("date", "real", "cv", "ols" )] %>%
  gather(key = "variable", value = "value", -date)

ggplot(df_gathered, aes(x = date, y = value)) + 
  geom_line(aes(color = variable), size = 1) +
  theme_minimal()
```


Observa-se que os métodos sem penalização resultam em menos resíduos e ajusta melhor o modelo, o que é esperado, já que não exerce nenhuma restrição ao ajuste. Percebemos que o lambda maior faz um ajuste menos adequado, já que exerce maior penalização, restringindo o valor does coeficientes. 


#### Lasso

A regressão utilizando Lasso é muito parecida com a regressão Ridge, mas neste caso penaliza-se a soma dos valores absolutos dos coeficientes, ao invés dos quadrados:

$$
\hat{\beta}_{lasso} = \arg \min_{\beta_0, \dots, \beta_k}  \Bigg\{\sum_{i=1}^n\left(y_i - \beta_0 - \sum_{j=1}^k\beta_j x_{ij}\right) + \lambda\sum_{j=1}^k|\beta_j|\Bigg\}
$$

Como resultado, temos coeficientes que são totalmente zerados, permitindo uma seleção de variáveis. 

Testando com os valores de $\lambda$ fixado em 0.1 e escolhido por cross-validated:
```{r}
lasso_cv <- cv.glmnet(X, y, alpha = 1, lambda = lambdas_to_try, standardize = F, nfolds = 10)
lambda <- lasso_cv$lambda.min

fit.LASSO.CV = glmnet(y=y,x=X, lambda = lambda, alpha = 1)
fit.LASSO.Fixed01 = glmnet(y=y,x=X, lambda = 0.1, alpha = 1)


#Coeficientes do LASSO
B.hat.LASSO.CV = as.vector(fit.LASSO.CV$beta)
B.hat.LASSO.Fixed01 = as.vector(fit.LASSO.Fixed01$beta)


#Intercepto do LASSO
B0.hat.LASSO.CV = fit.LASSO.CV$a0
B0.hat.LASSO.Fixed01 = fit.LASSO.Fixed01$a0

B.hat.LASSO.CV = c(B0.hat.LASSO.CV, B.hat.LASSO.CV)
B.hat.LASSO.Fixed01 = c(B0.hat.LASSO.Fixed01, B.hat.LASSO.Fixed01)
B.hat.RR.Fixed = c(B0.hat.RR.Fixed, B.hat.RR.Fixed)
names(B.hat.LASSO.CV) = names(B.hat.LASSO.Fixed01) = names(B.hat.OLS)

lambdas = cbind(lambda, 0.1)
coefs = cbind(B.hat.LASSO.CV, B.hat.LASSO.Fixed01)
rownames(lambdas) = "Lambda value"
kable(rbind(coefs, lambdas))
```

Observa-se que o método de cross-validation escolheu o $\lambda = 0.01$ e, em comparação com o $\lambda = 0.1$ zerou menos coeficientes, o que é esperado, pois quanto maior o $\lambda$, mais variáveis serão zeradas, como indicado no gráfico abaixo. 

```{r}
res <- glmnet(X, y, alpha = 1, lambda = lambdas_to_try, standardize = FALSE)
plot(res, xvar = "lambda")
legend("bottomright", lwd = 1, col = 1:6, legend = colnames(X), cex = .7)
```
As variáveis que indicam o valor de fechamento do índice S&P com lag = 1 e lag = 2 foram as mais significativas para a predição do valor de fechamento do dia atual. Com o $lambda = 0.1$, apenas estas variáveis formaram o modelo de final. Com o $\lambda = 0.01$, foram incluídas também as variáveis AMZN, AAPL, JPM, DJI e NYSE, sendo as duas últimas com coeficientes maiores, indicando que o valor dos outros índices possuem mais interferência no valor de fechamento do índice S&P do que ativos de empresas. 

```{r}
y_hat_cv = predict(fit.LASSO.CV, X)
ssr_cv = t(y - y_hat_cv) %*% (y - y_hat_cv)

y_hat_01 = predict(fit.LASSO.Fixed01, X)
ssr_01 = t(y - y_hat_01) %*% (y - y_hat_01)

y_hat = ts(cbind(y,  y_hat_cv, y_hat_01))
colnames(y_hat) = c("real", "cv", "fixed 0.1")
df_y <- data.frame(y_hat)
df_y$date <- as.numeric(row.names(df_y))

df_gathered = df_y %>%
  gather(key = "variable", value = "value", -date)

ggplot(df_gathered, aes(x = date, y = value)) + 
  geom_line(aes(color = variable), size = 1) +
  theme_minimal()

ssr = cbind(ssr_cv, ssr_01)
colnames(ssr) = c("cv", "fixed 0.1")
rownames(ssr) = "SSR"
kable(ssr)

```

O modelo com o $\lambda = 0.01$ obteve um melhor ajuste na predição.


#### AdaLasso

O método de AdaLasso introduz a ideia de adicionar pesos na penalização, de forma que valores altos de $\beta$ estimados recebem pesos menores e, consequentemente menores penalizações do que valores baixos de $\beta$. 

$$
\hat{\beta}_{adaLASSO} = \arg \min_{\beta_0, \dots, \beta_k}  \Bigg\{\sum_{i=1}^n\left(y_i - \beta_0 - \sum_{j=1}^k\beta_j x_{ij}\right) + \lambda\sum_{j=1}^k\hat\omega_j}|\beta_j|\Bigg\}
$$
onde $\hat{\mathbf{\omega}}=|\hat{\beta}|^{-\gamma}$, $\gamma >0$ e $\beta$ um estimador consistente, como os obtidos por Ridge ou OLS.

Implementando AdaLasso com $\lambda \simeq 0.01$ e $\gamma = 0.5$:
```{r}
#Calculo dos pesos utilizando os coeficientes do LASSO como Primeiro Estagio
n = nrow(X)
w = (abs(B.hat.LASSO.CV)+(n)^(-1/2))^(-1)

# Segundo Estagio do adaLASSO
fit.adaLASSO = glmnet(y=y,x=X, lambda = lambda, alpha = 1, penalty.factor = w)

#Coeficentes do adaLASSO
B.hat.adaLASSO = as.vector(fit.adaLASSO$beta)

#Intercepto do adaLASSO
B0.hat.adaLASSO = fit.adaLASSO$a0

B.hat.adaLASSO = c(B0.hat.adaLASSO, B.hat.adaLASSO)
names(B.hat.adaLASSO) = names(B.hat.OLS)

lasso_coefs = cbind(B.hat.adaLASSO, B.hat.LASSO.CV)
colnames(lasso_coefs) = c("adaLASSO", "LASSO")
lasso_coefs
```

Observa-se que com este método, a variável que indica o fechamento do índice S&P com lag = 2 foi removida, e o valor do ativo GE foi adicionad, em comparação com o método de regressão LASSO. 

```{r}
y_hat_lasso = predict(fit.LASSO.CV, X)
ssr_lasso = t(y - y_hat_lasso) %*% (y - y_hat_lasso)

y_hat_adalasso = predict(fit.adaLASSO, X)
ssr_adalasso = t(y - y_hat_adalasso) %*% (y - y_hat_adalasso)

y_hat = ts(cbind(y,  y_hat_lasso, y_hat_adalasso))
colnames(y_hat) = c("real", "lasso", "adaLasso")
df_y <- data.frame(y_hat)
df_y$date <- as.numeric(row.names(df_y))

df_gathered = df_y %>%
  gather(key = "variable", value = "value", -date)

ggplot(df_gathered, aes(x = date, y = value)) + 
  geom_line(aes(color = variable), size = 1) +
  theme_minimal()

ssr = cbind(ssr_lasso, ssr_adalasso)
colnames(ssr) = c("lasso", "adalasso")
rownames(ssr) = "SSR"
kable(ssr)
```

Para este caso, a regressão adaLASSO se mostrou levemente superior.

#### Trees

O método de árvores de decisão consiste em recursivamente encontrar a variável que será usada para particionar os dados remanescentes de forma que a soma do quadrado dos erros entre o valor real da variável resposta e a constante predita naquele grupo é minimizado.

```{r}
library(rpart)
tree <- rpart(
  formula = `close` ~ .,
  data    = df,
  method  = "anova",
  model = F,
  control = list(cp = 0)
)
plotcp(tree)
abline(v = 8, lty = "dashed")
tree$cptable
```

```{r}
library(rpart.plot)
rpart.plot(tree)
```

```{r}
library(vip)
vip(tree, num_features = 40, bar = FALSE)
```

##### Bagging


```{r}
library(ipred)
bag <- bagging(
  formula = close ~ .,
  data = df,
  nbagg = 100,  
  coob = TRUE,
  control = rpart.control(minsplit = 2, cp = 0),
  model = F
)
bag
```


##### Random Forest

```{r}
library(ranger)   # a c++ implementation of random forest 
library(h2o)      # a java-based implementation of random forest
```





```{r}
#mincut = 5
#minsize = 3*mincut
#mindev = 0.01
#control1 = tree.control(nobs=length(s), mincut = mincut, minsize = minsize, mindev = mindev)
#Ajuste da arvore de regressao
#tree1 = tree(formula=y~.,data = data, control=control1, subset = s)
```


## Notes

VAR:
https://www.quantstart.com/articles/Johansen-Test-for-Cointegrating-Time-Series-Analysis-in-R/
https://bookdown.org/ccolonescu/RPoE4/vec-and-var-models.html#estimating-a-vec-model
https://corporatefinanceinstitute.com/resources/knowledge/other/cointegration/#:~:text=A%20cointegration%20test%20is%20used,time%20in%20the%20long%20term.

arrtigo lidando com clima: https://www.kaggle.com/sumanthvrao/daily-climate-time-series-data
http://www.phdeconomics.sssup.it/documents/Lesson17.pdf

ref AIC BIC: https://www.datacamp.com/community/tutorials/tutorial-ridge-lasso-elastic-net


