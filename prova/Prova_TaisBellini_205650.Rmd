---
title: "PPGEst - Prova EST0203 - Séries Temporais - Verão 2021"
author: "Tais Bellini"
date: "4/10/2021"
output: pdf_document
header-includes:
  - \DeclareMathOperator{\EX}{\mathbb{E}}
  - \DeclareMathOperator{\Z}{\mathbb{Z}}
  - \DeclareMathOperator*{\argmax}{arg\,max}


---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Questão 1: Derivar as funções de autocorrelação:

### a) De $Y_t$ que é um ARMA(1,1) estacionário.

Seja $\{ X_t \}_{t \in Z}$ um processo estocástico ARMA(1,1) estacionário, podemos representá-lo da seguinte forma:

$$
Y_t = \phi_1 Y_{t-1} + \epsilon_t + \theta_1 \epsilon_{t-1}
$$
onde $Y_t = X_t - \mu$, $\mu = \EX{X_t}$, $\{\epsilon_t\}_{t \in Z}$ ~ $RB(0, \sigma^2)$.

A função de autocorrelação ($\rho_Y(h)$) se dá pela razão entre a função de autocovariância no lag h ($\gamma_Y(h)$) e a função de autocorrelação no lag 0 ($\gamma_Y(0)$):

$$
\rho_Y(h) = \frac{\gamma_Y(h)}{\gamma_Y(0)}
$$

Portanto, para calcular a funcão de autocorrelação, precisamos da função de autocovariância:

$$ \gamma_Y(h) = Cov(Y_t, Y_{t-h}) = Cov(Y_{t+h}, Y_t)$$
Abrindo $Y_{t+h}$, obtemos:

$$
\gamma_Y(h) = Cov[(\phi_1 Y_{t+h-1} + \epsilon_{t+h} + \theta_1 \epsilon_{t+h-1}),Y_t)]
$$
$$
= \phi_1 Cov(Y_{t+h-1},Y_t) + Cov(\epsilon_{t+h}, Y_t) + \theta_1 Cov(\epsilon_{t+h-1}, Y_t)
$$

Agora note que $Cov(Y_{t+h-1}, Y_t) = \gamma_Y(h-1)$. Assim, a função de autocorrelação $\gamma_Y(h)$ se dá por: 

$$
\gamma_Y(h) = \phi_1 \gamma_Y(h-1) + Cov(\epsilon_{t+h}, Y_t) + \theta_1 Cov(\epsilon_{t+h-1}, Y_t)
$$

Observe que, para $h > 1$, teremos os termos de covarância $Cov(\epsilon_{t+h}, Y_t)$ e $\theta_1 Cov(\epsilon_{t+h-1}, Y_t)$ zerados, pois o ruído branco do futuro não interfere em $Y_t$. Assim, neste caso, as funções de autocovariância e autocorrelacão de $Y_t$ que é um ARMA(1,1) estacionário se comportam como as de um AR(1): 

$$
\gamma_Y(h) = \phi_1\gamma_Y(h-1)
$$
$$
\rho_Y(h) = \phi_1\rho(h-1)
$$
onde $h>1$.

Já para $h \leq 1$, os termos de covariância não são zerados. Para $h=0$, temos:

\begin{equation}
\gamma_Y(0) = \phi_1 \gamma_Y(-1) + Cov(\epsilon_t, Y_t) + \theta_1Cov(\epsilon_{t-1}, Y_t)
\label{gamma0_raw}
\end{equation}

Sabemos que, quando $Y_t$ é estacionário, $\gamma_Y(-h) = \gamma_Y(h)$. Ainda, 

$$
Cov(\epsilon_t, Y_t) = Cov(\epsilon_t, (\phi_1Y_{t-1} + \epsilon_t + \theta_1\epsilon_{t-1}))
$$
$$
= \phi_1Cov(\epsilon_t, Y_{t-1}) + Cov(\epsilon_t, \epsilon_t) + \theta_1Cov(\epsilon_t, \epsilon_{t-1})
$$
onde $Cov(\epsilon_t,Y_{t-1}) = 0$, pois $Y_{t-1}$ não depende de $\epsilon$ no tempo presente. Como $\epsilon_t$ é ruído branco, $Cov(\epsilon_t, \epsilon_{t-1})=0$ e $Cov(\epsilon_t, \epsilon_t) = \sigma^2$ logo, podemos expressar

\begin{equation}
Cov(\epsilon_t, Y_t)  = \sigma^2
\label{cov_et_yt}
\end{equation}

Para determinarmos a covariância entre $\epsilon_{t-1}$ e $Y_t$, temos que:
\begin{equation}
Cov(\epsilon_{t-1}, Y_t) = \phi_1Cov(\epsilon_{t-1}, Y_{t-1}) + Cov(\epsilon_{t-1}, \epsilon_t) + \theta_1 Cov(\epsilon_{t-1}, \epsilon_{t-1})
\label{cov_et-1_yt_raw}
\end{equation}
onde $Cov(\epsilon_{t-1}, \epsilon_t) = 0$, $Cov(\epsilon_{t-1}, \epsilon_{t-1}) = \sigma^2$ e 
$$
Cov(\epsilon_{t-1}, Y_{t-1}) = Cov(\epsilon_{t-1}, (\phi_1Y_{t-2} + \epsilon_{t-1} + \theta_1\epsilon_{t-2})) = \phi_1Cov(\epsilon_{t-1}, Y_{t-2}) + Cov(\epsilon_{t-1},\epsilon_{t-1}) + \theta_1Cov(\epsilon_{t-1}, \epsilon_{t-2})
$$
no qual, pelas propriedades do processo e do ruído branco, $Cov(\epsilon_{t-1}, Y_{t-2}) = 0$, $Cov(\epsilon_{t-1}, \epsilon_{t-2}) = 0$ e $Cov(\epsilon_{t-1},\epsilon_{t-1}) = \sigma^2$. Portanto, 
\begin{equation}
Cov(\epsilon_{t-1}, Y_{t-1}) = \sigma^2
\label{cov_etmin1_ytmin1}
\end{equation}

Substituindo \ref{cov_etmin1_ytmin1} em \ref{cov_et-1_yt_raw} e considerando as propriedades do processo e de ruído branco, temos 
\begin{equation}
Cov(\epsilon_{t-1}, Y_t) = \phi_1\sigma^2 + \theta_1\sigma^2
\label{cov_et-1_yt}
\end{equation}

Finalmente, substituindo \ref{cov_et-1_yt} e \ref{cov_et_yt} em \ref{gamma0_raw}, temos:
$$
\gamma_Y(0) = \phi_1\gamma_Y(1) + \sigma^2 + \theta_1 (\phi_1 + \theta_1)\sigma^2
$$
Para $h=1$, determinamos que:

\begin{equation}
\gamma_Y(1) = \phi_1 \gamma_Y(0) + Cov(\epsilon_{t+1}, Y_t) + \theta_1 Cov(\epsilon_{t}, Y_t) = \phi_1 \gamma_Y(0) + \theta_1\sigma^2
\label{gamma1}
\end{equation}

Resolvendo $\gamma_Y(0)$ e $\gamma_Y(1)$ simultaneamente, concluímos que:

$$
\gamma_Y(0) = \frac{2\phi_1\theta_1 + 1 + \theta_1^2}{1-\phi_1^2}\sigma^2
$$
e
$$
\gamma_Y(1) =  \frac{(\phi_1^2\theta_1 + \theta_1 + \phi_1\theta_1^2 + \theta_1)\sigma^2}{1-\phi_1^2} = \frac{(1+\phi_1\theta_1)(\phi_1+\theta_1)}{1-\phi_1^2}\sigma^2
$$
Portanto, para $h \leq 1$,
$$
\rho_Y(0) = 1
$$

$$
\rho_Y(1) = \frac{\gamma_Y(1)}{\gamma_Y(0)} = \frac{(1+\phi_1\theta_1)(\phi_1+\theta_1)}{2\phi_1\theta_1 + 1 + \theta_1^2} 
$$
e para $h > 1$:
$$
\rho_Y(h) = \phi_1\rho(h-1)
$$

### b) De $Y_t^2$, onde $Y_t$ é um ARCH(2) estacionário.

Dado que $Y_t$ é um ARCH(2) estacionário, temos o seguinte modelo:

\begin{equation}
Y_t = \sigma_t\epsilon_t
\label{yt}
\end{equation}

onde $\epsilon_t \sim RB(0,1)$ e $\sigma_t^2$ é a variância condicional de $Y_t$ dadas as informações que temos até o instante $\Im_{t-1}$ que tem a seguinte estrutura:
\begin{equation}
\sigma^2 = \alpha_0 + \alpha_1Y_{t-1}^2 + \alpha_2Y_{t-2}^2
\label{sigma2}
\end{equation}

Para determinar a função de autocorrelação do $Y_t^2$, vamos elevar \ref{yt} ao quadrado em ambos os lados:

$$
Y_t^2 = \sigma_t^2\epsilon_t^2
$$

Somando e subtraindo $\sigma^2$ da equação, obtemos a seguinte estrutura:

$$
Y_t^2 = \alpha_0 + \alpha_1Y_{t-1}^2 + \alpha_2Y_{t-2}^2 + \sigma^2(\epsilon_t^2-1)
$$

Observe que $\EX{[\sigma_t^2(\epsilon_t^2-1)]} = 0$: 
$$
\EX{[\sigma_t^2(\epsilon_t^2-1)]} = \EX{\sigma_t^2}\EX{[\epsilon_t^2 - 1]}
$$
onde
$$
\EX{[\epsilon_t^2 - 1]} = \EX{\epsilon_t^2} - 1 = Var(\epsilon_t) + [\EX{\epsilon}]^2 - 1 = 1 + 0 -1 = 0
$$

Ainda, $Var(\sigma_t^2(\epsilon_t^2-1)) = Var(\epsilon_t^2-1)\EX{\sigma^2}^2$ e, sob a condição de que \ref{yt} e \ref{sigma2} têm quarto momento definido,  $Cov(\sigma_t^2(\epsilon_t^2-1), \sigma_{t-k}^2(\epsilon_{t-k}^2-1)) = 0$ para $k = 1, 2, \dots$. Assim, concluímos que $\sigma_t^2(\epsilon_t^2-1)$ é um ruído branco, e a estrutura de $Y_t^2$ se dá por: 

\begin{equation}
Y_t^2 = \alpha_0 + \alpha_1Y_{t-1}^2 + \alpha_2Y_{t-2}^2 + \varepsilon_t
\label{yt2}
\end{equation}
onde $EX{\varepsilon_t} = 0$ e $Var(\varepsilon_t) = Var(\epsilon_t^2-1)\EX{\sigma^2}^2 = \Sigma$.

Portanto, temos que $Y_t^2$ é um processo AR(2). Assim, sua função de autocorrelação ($\rho_{Y^2}$) é derivada da autocovariância ($\gamma_{Y^2}$):

$$
\gamma_{Y^2}(k) = \EX{[(Y_t^2- \mu), (Y_{t-k}^2 - \mu)]} = \EX{(Y_t^2Y_{t-k}^2)}
$$
$$
= \EX{[(\alpha_1Y_{t-1}^2 + \alpha_2Y_{t-2}^2 + \varepsilon_t)Y_{t-k}^2]}
$$
$$
= \alpha_1\EX{[Y_{t-1}^2Y_{t-k}^2]} + \alpha_2\EX{[Y_{t-2}^2Y_{t-k}^2]} + \EX{[\varepsilon_tY_{t-k}^2]}
$$
Ou seja, para $k=0$:
$$
\gamma_{Y^2}(k) = \alpha_1\gamma_{Y^2}(k-1) + \alpha_2\gamma_{Y^2}(k-2) + \Sigma
$$
E para $k>0$:
$$
\gamma_{Y^2}(k) = \alpha_1\gamma_{Y^2}(k-1) + \alpha_2\gamma_{Y^2}(k-2)
$$
Portanto, as condições iniciais se dão pela resolução das seguintes equações simultaneamente:

$$
\gamma_{Y^2}(0) = \alpha_1\gamma_{Y^2}(1) + \alpha_2\gamma_{Y^2}(2) + \Sigma
$$
$$
\gamma_{Y^2}(1) = \alpha_1\gamma_{Y^2}(0) + \alpha_2\gamma_{Y^2}(1)
$$
$$
\gamma_{Y^2}(2) = \alpha_1\gamma_{Y^2}(1) + \alpha_2\gamma_{Y^2}(0)
$$
Logo, temos:

$$
\gamma_{Y^2}(0) = \left(\frac{1-\alpha_2}{1+\alpha_2}\right)\frac{\Sigma}{(1-\alpha_2)^2-\alpha_1^2}
$$
$$
\gamma_{Y^2}(1) = \frac{\alpha_1}{1-\alpha_2}\gamma_{Y_t^2}(0) = \left(\frac{\alpha_1}{1-\alpha_2}\right)\left(\frac{1-\alpha_2}{1+\alpha_2}\right)\frac{\Sigma}{(1-\alpha_2)^2-\alpha_1^2}
$$
A partir disto, podemos derivar $\gamma_{Y^2}(k)$ para $k = 2, 3, \dots$:
$$
\gamma_{Y^2}(k) = \alpha_1\gamma_{Y^2}(k-1) + \alpha_2\gamma_{Y^2}(k-2)
$$
Finalmente, derivamos as funções de autocorrelação ($\rho_{Y_t^2}(k)$): 

$$
\rho_{Y_t^2}(1) = \frac{\gamma_{Y^2}(1)}{\gamma_{Y^2}(0)} = \frac{\alpha_1}{1-\alpha_2}
$$
$$
\rho_{Y_t^2}(2) = \alpha_1\rho_{Y_t^2}(1) + \alpha_2 = \frac{\alpha_1^2+(1-\alpha_2)\alpha_2}{1-\alpha_2}
$$
Para $k = 3, 4, \dots$:
$$
\rho_{Y_t^2}(k) = \alpha_1\rho_{Y_t^2}(k-1) + \alpha_2\rho_{Y_t^2}(k-2)
$$
## Questão 2

### a) Encontre a distribuição assintótica do estimador de máxima versossimilhança (EMV) de $\phi_1$ no modelo $y_t = \phi_0 + \phi_1y_{t-1} + \epsilon_t$ onde $\epsilon_t \sim N(0, \sigma^2)$ Use o resultado geral para os EMV's e explicite qualquer suposição adicional necessária.

Seja $\theta = (\phi_1, \dots, \phi_p, \sigma^2)$ o vetor dos parâmetros populacionais que deseja-se estimar e $\mathbf{Y} = (Y_1, \dots, Y_{T-1}, Y_T)$ um amostra observada. A função densidade de probabilidade (FDP) conjunta é denotada por:
\begin{equation}
f(Y_T, Y_{T-1} \dots, Y_1;\theta)
\label{fdp}
\end{equation}

Logo, a função de máxima verossimilhança é dada por:
\begin{equation}
L(\theta|\mathbf{Y}) = f(Y_T, Y_{T-1} \dots, Y_1;\theta)
\label{fmv}
\end{equation}

E o estimador de log-máxima verossimilhança (EMV) será o argumento que maximiza a função de máxima verossimilhança:
$$
\hat{\theta}_{EMV} = \arg \max_{\theta} l(\theta|\mathbf{Y})
$$
onde $l(\theta|\mathbf{y}) = ln(L(\theta|\mathbf{y}))$.

Assumindo que a derivada de $l(\theta|\mathbf{y})$ em relação a $\theta$ existe e é contínua, determinamos o $\hat{\theta}_{EMV}$ resolvendo:
$$
\frac{\delta l(\theta|\mathbf{Y})}{\delta \theta} = 0
$$
Agora, considerando o modelo AR(p):
\begin{equation}
Y_t = \phi_0 + \phi_1Y_{t-1} + \dots + \phi_pY_{t-p} + \epsilon_t
\label{arp}
\end{equation}
onde $\epsilon_t \sim i.i.d\ N(0, \sigma^2)$, e $Y_{t-1} \dots, Y_{t-p}$ valores do passado já observados, podemos considerar que $\phi_0 + \phi_1Y_{t-1} + \dots + \phi_pY_{t-p}$ é um valor constante que está sendo somado a $\epsilon_t$ e, portanto:
$$
Y_t|\Im_{t-1} \sim N(\phi_0 + \phi_1Y_{t-1} + \dots + \phi_pY_{t-p}, \sigma^2)
$$
Ainda, sabendo que uma FDP conjunta qualquer $f(X,Y)$ pode ser decomposta em $f(Y|X)f(X)$ e conhecendo a distribuicão de $Y_t|\Im_{t-1}$ podemos reescrever \ref{fmv} como:
$$
L(\theta|\mathbf{Y}) = f(Y_T|Y_1, \dots, Y_{T-1};\theta)f(Y_{T-1} \dots, Y_1;\theta)
$$
E assim podemos recursivamente decompor a função conjunta do segundo termo de forma a obter o produto das FDP condicionais de T-1 até 2, e ter FDP marginal de $Y_1$ no segundo termo.

Agora 

Ainda, no caso de $Y_1$, vamos retomar o fato de que, para um processo estacionário AR(1):
$$
\EX{Y_1} = \mu = \frac{\phi_0}{1-\phi_1}
$$
$$
Var(Y_1) = \frac{\sigma^2}{1-\phi_1^2}
$$
Portanto, temos que $Y_1 \sim N(\frac{\phi_0}{1-\phi_1}, \frac{\sigma^2}{1-\phi_1^2})$.

Olhando especificamente para o modelo AR(1):
\begin{equation}
Y_t = \phi_0 + \phi_1Y_{t-1} + \epsilon_t
\label{ar1}
\end{equation}

A decomposição da função de máxima verossimilhança é feita da seguinte forma:

$$
L(\theta|\mathbf{Y}) = f(Y_t|Y_{t-1};\theta)f(Y_{t-1};\theta)
$$
https://faculty.washington.edu/ezivot/econ584/notes/armaestimation.pdf
http://www.science.unitn.it/AnalisiInfoTLC/SSP/SSP14_15/lectures/lecture15.pdf
https://math.unice.fr/~frapetti/CorsoP/Chapitre_4_IMEA_1.pdf
http://www-stat.wharton.upenn.edu/~stine/stat910/lectures/12_est_arma.pdf
http://course.sdu.edu.cn/G2S/eWebEditor/uploadfile/20140110134920017.pdf

## Questão 3

### b) Com uma série temporal vetorial multivariada a sua escolha (mas com dimensão mínima de 3), estime um VAR ou VEC (o que for mais adequado – faça testes),mostrando os resultados e os interpretando.

Para esta questão, será usado um banco de dados de 1462 observações que mede o clima diário da cidade de Delhi, na Índia, no período de primeiro de Janeiro de 2013 a 24 de Abril de 2017 medindo os seguintes parâmetros: temperatura média, umidade, velocidade do vento e pressão média do ar. 

```{r, echo = F, include = F}
library(tseries) # for ADF unit root tests
library(urca)
library(vars)
library(mFilter)
library(forecast)
```

Análise exploratória dos dados:

```{r}
data = read.csv("DailyDelhiClimateTrain.csv", sep=",")
data = data[data$meanpressure > 990 & data$meanpressure < 1300,] #remove outliers
data = data[data$wind_speed < 30,] #remove outliers
ts_data = ts(data)
temp = ts(ts_data[,2])
humidity = ts(ts_data[,3])
wind = ts(ts_data[,4])
pressure = ts(ts_data[,5])
plot(scale(ts_data[,-1]), main = "Plot de cada série do banco de dados",  yax.flip = TRUE)
```

Para uma análise de séries temporais multivariadas, temos o modelo VAR, cuja fórmula geral VAR(p) tem a seguinte estrutura:

$$
Y_t = \mathbf{\mu} + A_1\mathbf{Y_{t-1}} + \dots + A_p\mathbf{Y_{t-p}} + \mathbf{\epsilon_t}
$$
onde $\mu$ é o vetor de médias da série, $A_i$ são as matrizes dos coeficientes para cada lag e $\epsilon_t$  é vetor de erros com distribuição normal multivariada com média zero. 

Para utilizá-lo, é necessário avaliar se as séries a serem trabalhadas são estacionárias:

```{r, warning=F}
par(mfrow=c(2,2))
acf(temp, main = "ACF Temperatura Média")
acf(humidity, main = "ACF Umidade")
acf(wind, main = "ACF Velocidade do vento")
acf(pressure, main = "ACF Pressão do ar Média")

par(mfrow=c(2,2))
pacf(temp)
pacf(humidity)
pacf(wind)
pacf(pressure)


t = adf.test(temp)
t = adf.test(diff(temp))
h = adf.test(humidity)
w = adf.test(wind)
p = adf.test(pressure)
p = adf.test(diff(pressure))

adf_pvalues = c(t$p.value, h$p.value, w$p.value, p$p.value)
names(adf_pvalues) = c("Temp", "Umidade", "Vento", "Pressao")
paste("P-Value do teste ADF")
adf_pvalues
```

Observamos que apenas a série da temperatura média possui p-valor > 0.05, mas ela se torna estacionária ao tomar uma diferença. Assim, utilizaremos esta variável com uma diferença para modelar o VAR.



```{r}
autoplot(cbind(temp, humidity, wind, scale(pressure)))

ts_data_I = cbind(diff(temp), humidity, wind, diff(pressure))
jo = ca.jo(ts_data_I)
summary(jo)
ts_data_I = ts_data_I[-1,]

fit = lm(diff(temp)~diff(pressure))
summary(fit)
plot(residuals(fit))

VARselect(ts_data_I, lag.max = 40, type="const")

model = VAR(ts_data_I, p=1, type="const")
summary(model)

serial_model = serial.test(model, lags.pt = 15)
serial_model

arch.test(model, lags.multi = 15, multivariate.only = T)

normality.test(model, multivariate.only = T)
plot(stability(model))


```

Estimação

```{r}
library(tsDyn)
model = VECM(ts_data[,-1], 3, r = 2, estim = "ML")
summary(model)
```


Diagnostico

```{r}
varmod = vec2var(jo, r = 2)

ser = serial.test(varmod, lags.pt = 4)
ser

arch.test(varmod, lags.multi = 15, multivariate.only = T)

normality.test(varmod, multivariate.only = T)
```


```{r}
```


```{r, warning=F}
par(mfrow=c(2,2))
for(i in 2:5){
  acf(ts_data[,i])
}

t = adf.test(temp)
t = adf.test(diff(temp))
h = adf.test(humidity)
w = adf.test(wind)
p = adf.test(pressure)

adf_pvalues = c(t$p.value, h$p.value, w$p.value, p$p.value)
names(adf_pvalues) = c("Temp", "Umidade", "Vento", "Pressao")
adf_pvalues
```

Observa-se que apenas 2 variáveis são séries estacionárias. Portanto, vamos testar a co-integração através do método dos autovalores de Johansen. 

Para isto, temos o modelo VEC, que faz a diferenciação das séries obtendo a seguinte forma:

$$
\Delta \mathbf{Y_t} = \mathbf{\mu} + A\mathbf{Y_{t-1}} + \Gamma_1\Delta \mathbf{Y_{t-1}} + \dots + \Gamma_p\Delta \mathbf{Y_{t-p}} + \mathbf{\epsilon_t}
$$
onde $\Delta \mathbf{Y_{t}} = \mathbf{Y_t} - \mathbf{Y_{t-1}}$ é o operador de diferenciação, A é a matriz de coeficientes para o primeiro lag e $\Gamma_i$ as matrizes dos coeficientes para cada lag. 

O teste de Johansen verifica sequencialmente se o posto da matriz é 0, 1, 2, ... até o número de séries sendo avaliadas menos um. Se o posto da matriz A = 0, não há co-integração.

```{r}
jotest = ca.jo(ts_data[,-1], spec = "longrun")
summary(jotest)

lambda = c(0.245989042, 0.063838185, 0.009330528)/0.331000093

temperature = lambda[1]*humidity + lambda[2]*wind + lambda[3]*pressure
plot(temperature, type = "l")
adf.test(temperature)
```





## Questão 4: Utilizando uma série temporal a sua escolha (mas com pelo menos 10 séries temporais como covariáveis e pelo menos 3 lags de cada uma delas, ou seja, pelo menos 30 covariáveis), implemente, para estimação e seleção, todos métodos vistos em aula sobre machine/statistical learning: ridge, lasso, adalasso, tress (baging, random forest e boosting). Mostre os resultados e os interprete.

Para esta questão, será utilizado um banco de dados com dados do comportamento do tráfego de veículos em São Paulo. O banco tem observações de um período de 14 de Dezembro de 2009 até 18 de Dezembro de 2009 (segunda-feira a sexta-feira), das 7:00 até as 20:00 de 30 em 30 minutos. Os dados contam com a observação de 16 variáveis que contam o número de ocorrências das seguintes situações: ônibus parados, caminhões quebrados, excesso de veículos, acidentes com vítimas, atropelamentos, veículos de bombeiros, ocorrências com carga, acidentes com cargas perigosas, falta de luz, incêndios, alagamentos, manifestações, erro na rede de ônibus elétricos, árvore na pista, semáforo desligado e semáforo intermitente. Ainda, possui a variável lentidão do tráfego, em porcentagem.

```{r}
library(ggplot2)
library(tidyr)
library(dplyr)
```


```{r}
data = read.csv2("urban_traffic_sp.csv", sep = ";")
colnames(data) = c("Hour", "ImmobilizedBus", "BrokenTruck", "VehicleExcess", 
                   "AccidentVictim", "RunningOver" , "FireVehicles", 
                   "OccurrenceFreight", "IncidentDangerousFreight",
                   "LackOfElectricity", "Fire", "Flooding", "Manifestations",
                   "DefectNetworkTrolleybuses", "TreeOnTheRoad", "SemaphoreOff",
                   "IntermittentSemaphore", "SlownessinTraffic(%)")

ts_data = ts(data)

plot(ts_data[,c(18, 2:4)], main = "Plot de cada série do banco de dados",  yax.flip = TRUE)
plot(ts_data[,5:8], main = "Plot de cada série do banco de dados",  yax.flip = TRUE)
plot(ts_data[,9:12], main = "Plot de cada série do banco de dados",  yax.flip = TRUE)
plot(ts_data[,13:15], main = "Plot de cada série do banco de dados",  yax.flip = TRUE)
plot(ts_data[,16:17], main = "Plot de cada série do banco de dados",  yax.flip = TRUE)

```

#### Ridge

A regressão Ridge é uma regressão de mínimos quadrados ordinários, mas que realiza o _shrinkage_ dos coeficientes a partir de uma penalização na soma dos seus quadrados, impondo que esta soma seja menos que um valor definido $t$. Assim, temos que: 

$$
\hat{\beta}_{ridge} = \arg \min_{\beta_0, \dots, \beta_k}  \Bigg\{\sum_{i=1}^n\left(y_i - \beta_0 - \sum_{j=1}^k\beta_j x_{ij}\right) + \lambda\sum_{j=1}^k\beta_j^2\Bigg\}
$$

O resultado são coeficientes com valores menores do que uma regressão de mínimos quadrados tradicional (OLS).

Vamos determinar a variável resposta e centralizá-la, já que o algoritmo de regressão Ridge parte do pressuposto que os preditores estão padronizados e resposta centralizada. As covariáveis do banco já estão na mesma unidade, não sendo necessária a padronização.

```{r, echo=F, warning=F, include=F}
library(glmnet)
library(knitr)
library(psych) 
```

```{r}
# variavel resposta:
y = scale(data$`SlownessinTraffic(%)`, center = T, scale = F)

#covariaveis
X = as.matrix(data[,-c(1,18)])
```


Vamos observar como fica o ajuste em uma regressão de mínimos quadrados tradicional:
```{r}
#OLS
fit.OLS = lm(formula = y~X)
B.hat.OLS = fit.OLS$coefficients
kable(B.hat.OLS)
```

Agora para executar a regressão ridge, vamos escolher o $\lambda$ ótimo a partir dos critérios AIC e BIC:
```{r}
lambdas_to_try = 10^seq(-2, 3, length.out = 100)
aic <- c()
bic <- c()

for (lambda in seq(lambdas_to_try)) {
  # Não padronizamos os dados pois estão na mesma unidade (num de ocorrências)
  model <- glmnet(X, y, alpha = 0, lambda = lambdas_to_try[lambda], standardize = F)
  
  # Coeficientes e residuos (sem o intercepto)
  betas <- as.vector((as.matrix(coef(model))[-1, ]))
  resid <- y - (X %*% betas)
  
  # Computando AIC e BIC
  ld <- lambdas_to_try[lambda] * diag(ncol(X))
  H <- X %*% solve(t(X) %*% X + ld) %*% t(X)
  df <- tr(H)
  
  aic[lambda] <- nrow(X) * log(t(resid) %*% resid) + 2 * df
  bic[lambda] <- nrow(X) * log(t(resid) %*% resid) + 2 * df * log(nrow(X))
}

# Plot information criteria against tried values of lambdas
plot(log(lambdas_to_try), aic, col = "orange", type = "l", ylab = "Information Criterion",
     ylim = c(1020, 1158))
lines(log(lambdas_to_try), bic, col = "skyblue3")
legend("bottomright", lwd = 1, col = c("orange", "skyblue3"), legend = c("AIC", "BIC"))
```

Executando a regressão ridge com lambda fixo, e os lambdas escolhidos por AIC, BIC e cross-validation:

``` {r}
lambda_aic = lambdas_to_try[which.min(aic)]
lambda_bic = lambdas_to_try[which.min(bic)]
lambda_cv = cv.glmnet(X, y, alpha = 0, lambda = lambdas_to_try, standardize = F, nfolds = 10)$lambda.min
lambda_fixed = 5

#Ridge Regression - alpha = 0 - AIC
fit.RR.AIC = glmnet(y=y,x=X, lambda=lambda_aic, alpha = 0)
fit.RR.BIC = glmnet(y=y,x=X, lambda=lambda_bic, alpha = 0)
fit.RR.CV = glmnet(y=y,x=X, lambda=lambda_cv, alpha = 0)
fit.RR.Fixed = glmnet(y=y,x=X, lambda=lambda_fixed, alpha = 0)

#Coeficientes do RR
B.hat.RR.AIC = as.vector(fit.RR.AIC$beta)
B.hat.RR.BIC = as.vector(fit.RR.BIC$beta)
B.hat.RR.CV = as.vector(fit.RR.CV$beta)
B.hat.RR.Fixed = as.vector(fit.RR.Fixed$beta)

#Intercepto do RR
B0.hat.RR.AIC = fit.RR.AIC$a0
B0.hat.RR.BIC = fit.RR.BIC$a0
B0.hat.RR.CV = fit.RR.CV$a0
B0.hat.RR.Fixed = fit.RR.Fixed$a0

B.hat.RR.AIC = c(B0.hat.RR.AIC, B.hat.RR.AIC)
B.hat.RR.BIC = c(B0.hat.RR.BIC, B.hat.RR.BIC)
B.hat.RR.CV = c(B0.hat.RR.CV, B.hat.RR.CV)
B.hat.RR.Fixed = c(B0.hat.RR.Fixed, B.hat.RR.Fixed)
names(B.hat.RR.AIC) = names(B.hat.RR.BIC) = names(B.hat.RR.CV) = names(B.hat.RR.Fixed) = names(B.hat.OLS)

lambdas = cbind(0, lambda_aic, lambda_bic, lambda_cv, lambda_fixed)
coefs = cbind(B.hat.OLS, B.hat.RR.AIC, B.hat.RR.BIC, B.hat.RR.CV, B.hat.RR.Fixed)
rownames(lambdas) = "Lambda value"
rbind(coefs, lambdas)

```

Conseguimos observar que o $\lambda$ escolhido pelo critério AIC é bem pequeno (0.01), resultando em valores muito similares da regressão tradicional. Já o $\lambda$ escolhido pelo critério BIC é bem grande, resultando em coeficientes muito pequenos. Os $\lambda$ escolhidos por cross-validation e o fixado no valor 5 reduziram moderadamente os coeficientes. Estes resultados eram esperados, já que o valor dos coeficientes diminui a medida que o $\lambda$ aumenta, como observado no gráfico abaixo.


```{r}
res <- glmnet(X, y, alpha = 0, lambda = lambdas_to_try, standardize = FALSE)
plot(res, xvar = "lambda")
```
Agora, vamos avaliar o ajuste do modelo obtido de acordo com cada lambda escolhido:

```{r}
y_hat_ols = predict(fit.OLS)
ssr_ols = t(y - y_hat_ols) %*% (y - y_hat_ols)

y_hat_aic = predict(fit.RR.AIC, X)
ssr_aic = t(y - y_hat_aic) %*% (y - y_hat_aic)

y_hat_bic = predict(fit.RR.BIC, X)
ssr_bic = t(y - y_hat_bic) %*% (y - y_hat_bic)

y_hat_cv = predict(fit.RR.CV, X)
ssr_cv = t(y - y_hat_cv) %*% (y - y_hat_cv)

y_hat_fixed = predict(fit.RR.Fixed, X)
ssr_fixed = t(y - y_hat_fixed) %*% (y - y_hat_fixed)


y_hat = ts(cbind(y,  y_hat_ols, y_hat_aic, y_hat_bic, y_hat_cv, y_hat_fixed))
colnames(y_hat) = c("real", "ols", "aic", "bic", "cv", "fixed")
df <- data.frame(y_hat)
df$date <- as.numeric(row.names(df))


df_gathered = df %>%
  gather(key = "variable", value = "value", -date)

ggplot(df_gathered, aes(x = date, y = value)) + 
  geom_line(aes(color = variable), size = 1) +
  theme_minimal()

ssr = cbind(ssr_ols,ssr_aic, ssr_bic, ssr_cv, ssr_fixed)
colnames(ssr) = c("ols", "aic", "bic", "cv", "fixed")
rownames(ssr) = "SSR"
kable(ssr)
```

```{r}
df_gathered = df[,c("date", "real", "cv", "fixed" )] %>%
  gather(key = "variable", value = "value", -date)

ggplot(df_gathered, aes(x = date, y = value)) + 
  geom_line(aes(color = variable), size = 1) +
  theme_minimal()

```


Observa-se que o método sem penalização e o método com o lambda bem pequeno (escolhido via AIC) resulta em menos resíduos e ajusta melhor o modelo, o que é esperado, já que praticamente não exerce nenhuma penalização. No outro extremo, temos o lambda muito grande, escolhido via BIC, que ajusta muito mal os dados pois praticamente zera os coeficiente. Através de cross-validation e um lambda arbitrário, se obtém um ajuste um pouco pior mas com _shrinkage_ dos coeficientes, que era o nosso objetivo, indicando que um lambda entre 0.1 e 5 seja mais adequado para este modelo.


#### Lasso

A regressão utilizando Lasso é muito parecida com a regressão Ridge, mas neste caso penaliza-se a soma dos valores absolutos dos coeficientes, ao invés dos quadrados:

$$
\hat{\beta}_{lasso} = \arg \min_{\beta_0, \dots, \beta_k}  \Bigg\{\sum_{i=1}^n\left(y_i - \beta_0 - \sum_{j=1}^k\beta_j x_{ij}\right) + \lambda\sum_{j=1}^k|\beta_j|\Bigg\}
$$

Como resultado, temos coeficientes que são totalmente zerados, permitindo uma seleção de variáveis. 

Testando com os valores de $\lambda$ fixado 0.5, 0.1 e escolhido por cross-validated:
```{r}
lasso_cv <- cv.glmnet(X, y, alpha = 1, lambda = lambdas_to_try, standardize = F, nfolds = 10)
lambda <- lasso_cv$lambda.min

fit.LASSO.CV = glmnet(y=y,x=X, lambda = lambda, alpha = 1)
fit.LASSO.Fixed01 = glmnet(y=y,x=X, lambda = 0.1, alpha = 1)
fit.LASSO.Fixed05 = glmnet(y=y,x=X, lambda = 0.5, alpha = 1)


#Coeficientes do LASSO
B.hat.LASSO.CV = as.vector(fit.LASSO.CV$beta)
B.hat.LASSO.Fixed01 = as.vector(fit.LASSO.Fixed01$beta)
B.hat.LASSO.Fixed05 = as.vector(fit.LASSO.Fixed05$beta)


#Intercepto do LASSO
B0.hat.LASSO.CV = fit.LASSO.CV$a0
B0.hat.LASSO.Fixed01 = fit.LASSO.Fixed01$a0
B0.hat.LASSO.Fixed05 = fit.LASSO.Fixed05$a0

B.hat.LASSO.CV = c(B0.hat.LASSO.CV, B.hat.LASSO.CV)
B.hat.LASSO.Fixed01 = c(B0.hat.LASSO.Fixed01, B.hat.LASSO.Fixed01)
B.hat.LASSO.Fixed05 = c(B0.hat.LASSO.Fixed05, B.hat.LASSO.Fixed05)
B.hat.RR.Fixed = c(B0.hat.RR.Fixed, B.hat.RR.Fixed)
names(B.hat.LASSO.CV) = names(B.hat.LASSO.Fixed01) = names(B.hat.LASSO.Fixed05) = names(B.hat.OLS)

lambdas = cbind(lambda, 0.1, 0.5)
coefs = cbind(B.hat.LASSO.CV, B.hat.LASSO.Fixed01, B.hat.LASSO.Fixed05)
rownames(lambdas) = "Lambda value"
rbind(coefs, lambdas)
```

Observa-se que o método de cross-validation escolheu o $\lambda = 0.1$ e, em comparação com o $\lambda = 0.5$ zerou menos coeficientes, o que é esperado, pois quanto maior o $\lambda$, mais variáveis serão zeradas, como indicado no gráfico abaixo. 

```{r}
res <- glmnet(X, y, alpha = 1, lambda = lambdas_to_try, standardize = FALSE)
plot(res, xvar = "lambda")
legend("bottomright", lwd = 1, col = 1:6, legend = colnames(X), cex = .7)
```
As variáveis que indicam ocorrência de caminhão parado, atropelamento e acidentes com carga foram removidas do modelo com o $\lambda$ menor. Com exceção de atropelamento, as outras variáveis possuem poucas ocorrências no banco de dados, sendo coerente a remoção delas.
Com o $\lambda =0.5$, apenas as variáveis que indicam veículos de bombeiros, semáforo desligado, alagamento, erro na rede de ônibus elétrico e falta de luz foram considerados. 

```{r}
y_hat_cv = predict(fit.LASSO.CV, X)
ssr_cv = t(y - y_hat_cv) %*% (y - y_hat_cv)

y_hat_05 = predict(fit.LASSO.Fixed05, X)
ssr_05 = t(y - y_hat_05) %*% (y - y_hat_05)

y_hat = ts(cbind(y,  y_hat_cv, y_hat_05))
colnames(y_hat) = c("real", "cv", "fixed 0.5")
df <- data.frame(y_hat)
df$date <- as.numeric(row.names(df))

df_gathered = df %>%
  gather(key = "variable", value = "value", -date)

ggplot(df_gathered, aes(x = date, y = value)) + 
  geom_line(aes(color = variable), size = 1) +
  theme_minimal()

ssr = cbind(ssr_cv, ssr_05)
colnames(ssr) = c("cv", "fixed 0.5")
rownames(ssr) = "SSR"
kable(ssr)

```

O modelo com o $\lambda = 0.1$ obteve um melhor ajuste na predição.


#### AdaLasso

O método de AdaLasso introduz a ideia de adicionar pesos na penalização, de forma que valores altos de $\beta$ estimados recebem pesos menores e, consequentemente menores penalizações do que valores baixos de $\beta$. 

$$
\hat{\beta}_{adaLASSO} = \arg \min_{\beta_0, \dots, \beta_k}  \Bigg\{\sum_{i=1}^n\left(y_i - \beta_0 - \sum_{j=1}^k\beta_j x_{ij}\right) + \lambda\sum_{j=1}^k\hat\omega_j}|\beta_j|\Bigg\}
$$
onde $\hat{\mathbf{\omega}}=|\hat{\beta}|^{-\gamma}$, $\gamma >0$ e $\beta$ um estimador consistente, como os obtidos por Ridge ou OLS.

Implementando AdaLasso com $\lambda \simeq 0.1$ e $\gamma = 0.5$:
```{r}
#Calculo dos pesos utilizando os coeficientes do LASSO como Primeiro Estagio
n = nrow(X)
w = (abs(B.hat.LASSO.CV)+(n)^(-1/2))^(-1)

# Segundo Estagio do adaLASSO
fit.adaLASSO = glmnet(y=y,x=X, lambda = lambda, alpha = 1, penalty.factor = w)

#Coeficentes do adaLASSO
B.hat.adaLASSO = as.vector(fit.adaLASSO$beta)

#Intercepto do adaLASSO
B0.hat.adaLASSO = fit.adaLASSO$a0

B.hat.adaLASSO = c(B0.hat.adaLASSO, B.hat.adaLASSO)
names(B.hat.adaLASSO) = names(B.hat.OLS)

lasso_coefs = cbind(B.hat.adaLASSO, B.hat.LASSO.CV)
colnames(lasso_coefs) = c("adaLASSO", "LASSO")
lasso_coefs
```

Observa-se que com este método, a variável excesso de veículos foi removida, e ocorrências com cargas foi adicionada, em comparação com o método de regressão LASSO. Em ambas, caminhões quebrados e acidentes com cargas perigosas não foram considerados.

```{r}
y_hat_lasso = predict(fit.LASSO.CV, X)
ssr_lasso = t(y - y_hat_lasso) %*% (y - y_hat_lasso)

y_hat_adalasso = predict(fit.adaLASSO, X)
ssr_adalasso = t(y - y_hat_adalasso) %*% (y - y_hat_adalasso)

y_hat = ts(cbind(y,  y_hat_lasso, y_hat_adalasso))
colnames(y_hat) = c("real", "lasso", "adaLasso")
df <- data.frame(y_hat)
df$date <- as.numeric(row.names(df))

df_gathered = df %>%
  gather(key = "variable", value = "value", -date)

ggplot(df_gathered, aes(x = date, y = value)) + 
  geom_line(aes(color = variable), size = 1) +
  theme_minimal()

ssr = cbind(ssr_lasso, ssr_adalasso)
colnames(ssr) = c("lasso", "adalasso")
rownames(ssr) = "SSR"
kable(ssr)
```

Para este caso, a regressão LASSO se mostrou levemente superior.

#### Trees

## Notes

VAR:
https://www.quantstart.com/articles/Johansen-Test-for-Cointegrating-Time-Series-Analysis-in-R/
https://bookdown.org/ccolonescu/RPoE4/vec-and-var-models.html#estimating-a-vec-model
https://corporatefinanceinstitute.com/resources/knowledge/other/cointegration/#:~:text=A%20cointegration%20test%20is%20used,time%20in%20the%20long%20term.

arrtigo lidando com clima: https://www.kaggle.com/sumanthvrao/daily-climate-time-series-data
http://www.phdeconomics.sssup.it/documents/Lesson17.pdf

ref AIC BIC: https://www.datacamp.com/community/tutorials/tutorial-ridge-lasso-elastic-net


